{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-31T18:43:42.999695Z",
     "start_time": "2018-05-31T18:43:40.515711Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done1\n",
      "stop\n",
      "stop\n",
      "done1\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "from os import listdir,mkdir\n",
    "from os.path import isfile, join, isdir,exists\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "from myplots import roundup, rounddown, find_decimal_fold, percentile_cut_off, rarefaction_calc, rarefaction_plot,draw_correlation_scatter\n",
    "from matplotlib.ticker import FormatStrFormatter\n",
    "import cPickle as pickle\n",
    "import seaborn as sns\n",
    "import random\n",
    "from scipy.stats import pearsonr\n",
    "from MyFunctionsShani import *\n",
    "import math\n",
    "from myplots import roundup, rounddown, find_decimal_fold\n",
    "from ShaniBA.GeneralFeaturePhenotypeInteractions.Feature_phenotype_functions import common_processing_feature_phenotype_matrices, editSampleNames,process_sample_matrix,compute_distance_matrix_general\n",
    "from SampleFileFunctions import *\n",
    "\n",
    "from tunneltoamazondb import getengine\n",
    "from pandas import concat, read_csv, Series, read_sql\n",
    "\n",
    "\n",
    "MyPath='/net/mraid08/export/genie/Lab/Personal/ShaniBAF'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-31T18:43:43.012945Z",
     "start_time": "2018-05-31T18:43:43.003106Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'31052018'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time\n",
    "cdate=str(time.strftime(\"%d%m%Y\"))\n",
    "cdate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# final function to generate BD_FD sample conversion file:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function updated=16.3.18 Change to matching BD-FD samples based on UserID only method! enables a little bit more samples with postHGF info\n",
    "this function was copied to SampleFileFunctions.py - USE AND UPDATE THERE!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_BD_FD_conversion_file(mergeOn,readCountFolder):\n",
    "    \n",
    "    import time\n",
    "    cdate=str(time.strftime(\"%d%m%Y\"))\n",
    "    cdate\n",
    "    \n",
    "    ## extract blood DNA information:\n",
    "    BDsamples = read_sql('select  DnaID,UserID,storageDT from Lab.hostdna', getengine())\n",
    "    BDsamples=BDsamples.rename(columns={'UserID':'oldUserID'})\n",
    "    BDsamples['DnaID_num']=BDsamples['DnaID'].str.replace('BD','').astype(np.float)\n",
    "    emptyBDs=BDsamples[BDsamples['DnaID'].isnull()]\n",
    "\n",
    "    print 'extracted list of  %s BD samples from DB' %len(BDsamples)\n",
    "    print 'last BD index is %s' %BDsamples['DnaID_num'].max()\n",
    "\n",
    "    BDsamples=BDsamples[BDsamples['DnaID'].notnull()] #remove rows with no BD\n",
    "    print 'number of rows without BD=%s-those rows were removed' %len(emptyBDs)\n",
    "\n",
    "    countingBDs=pd.DataFrame(BDsamples['DnaID_num'].value_counts()) #counting repeating BDs\n",
    "    repeatingBDs=countingBDs[countingBDs['DnaID_num']>1]\n",
    "    BDsamples=BDsamples.drop_duplicates(subset='DnaID') #removing BD duplicates\n",
    "    print 'BD samples repeating more than once=%s, duplicates were removed' %list(repeatingBDs.index)\n",
    "\n",
    "    print 'final number of rows should be the initial list minus empty BD rows and minus duplicate BD rows\\nand equall to the last BD index'\n",
    "    print 'final number of rows is %s' %len(BDsamples)\n",
    "    \n",
    "    ## correct UserIDs using correct RegNums:\n",
    "    #open the file with corrected userIDs for each BD number based on Daphna's correction for registration number:\n",
    "    file1='/net/mraid08/export/genie/Lab/Personal/ShaniBAF/Sample files/CleanCorrectBloodDNASamples'\n",
    "    CleanCorrectBloodDNASamples=pd.read_pickle(file1)\n",
    "    BD_correctReg=CleanCorrectBloodDNASamples[['DnaID','original registration code','correct registration code','correction status (reg number)']]\n",
    "\n",
    "    \n",
    "    # merge BD list with correct RegNum list:\n",
    "    BDwithCorrectReg=pd.merge(BDsamples,BD_correctReg,how='left',left_on='DnaID',right_on='DnaID')\n",
    "    BDwithCorrectReg['correct registration code']=BDwithCorrectReg['correct registration code'].fillna('unknown')\n",
    "    BDwithCorrectReg['correction status (reg number)']=BDwithCorrectReg['correction status (reg number)'].fillna('unknown')\n",
    "    print 'original BD sample list length is %s' %len(BDsamples)\n",
    "    print 'sample list length after merging is %s (SHOULD BE THE SAME AS ABOVE!)' %len(BDwithCorrectReg)\n",
    "\n",
    "    # extract UserID-regNum from the database\n",
    "    UserIDs_RegNum=read_sql('select UserID,RegistrationCode from pnp.users', getengine())\n",
    "\n",
    "    #merge new UserID to BD sample according to the corrected regNum\n",
    "    BDwithCorrectUserID=pd.merge(BDwithCorrectReg,UserIDs_RegNum,how='left', \n",
    "                                 left_on='correct registration code',\n",
    "                                right_on='RegistrationCode')\n",
    "    #give old UserIDs and regNum to samples that don't have correct regNum:\n",
    "    noCorrectReg=len(BDwithCorrectUserID[BDwithCorrectUserID['correct registration code']=='unknown'])\n",
    "    print '%s samples dont have correct registration code and are given old UserID' %noCorrectReg\n",
    "    BDwithCorrectUserID['UserID']=np.where(BDwithCorrectUserID['correct registration code']=='unknown',\n",
    "                                      BDwithCorrectUserID['oldUserID'],BDwithCorrectUserID['UserID'])\n",
    "\n",
    "\n",
    "    print 'sample number is still %s' %len(BDwithCorrectUserID)\n",
    "\n",
    "    BDwithCorrectUserID=BDwithCorrectUserID[['DnaID','storageDT', 'UserID','correct registration code','correction status (reg number)']]\n",
    "\n",
    "    #add regNum according to userID and use it only for samples with unknown regNum:\n",
    "    BDwithCorrectUserIDandRegs=pd.merge(BDwithCorrectUserID,UserIDs_RegNum,how='left',\n",
    "                                       left_on='UserID',right_on='UserID')\n",
    "    BDwithCorrectUserIDandRegs['correct registration code']=np.where(BDwithCorrectUserIDandRegs['correct registration code']=='unknown',\n",
    "                                      BDwithCorrectUserIDandRegs['RegistrationCode'],BDwithCorrectUserIDandRegs['correct registration code'])\n",
    "\n",
    "    BDwithCorrectUserIDandRegs=BDwithCorrectUserIDandRegs.drop('RegistrationCode',axis=1)\n",
    "    \n",
    "    ## generate BD_year and date:\n",
    "    BDwithCorrectUserIDandRegs['storageDT']=BDwithCorrectUserIDandRegs['storageDT'].astype(str)\n",
    "    BDwithCorrectUserIDandRegs['Blood_Year']=BDwithCorrectUserIDandRegs['storageDT'].str.split('-').str[0]\n",
    "    BDwithCorrectUserIDandRegs['Blood_Date']=BDwithCorrectUserIDandRegs['storageDT'].str.split(' ').str[0]\n",
    "    BDwithCorrectUserIDandRegs['UserID']=BDwithCorrectUserIDandRegs['UserID'].astype(str)\n",
    "    BDwithCorrectUserIDandRegs['UserID']=BDwithCorrectUserIDandRegs['UserID'].str.split('.').str[0]\n",
    "    BDwithCorrectUserIDandRegs['UserID_bYear']=BDwithCorrectUserIDandRegs['UserID'].astype(str).str.cat(BDwithCorrectUserIDandRegs['Blood_Year'],sep='_')\n",
    "    BDwithCorrectUserIDandRegs['UserID_bDate']=BDwithCorrectUserIDandRegs['UserID'].astype(str).str.cat(BDwithCorrectUserIDandRegs['Blood_Date'],sep='_')\n",
    "\n",
    "    ## extract FD table\n",
    "    FDsamples = read_sql('select  DnaID,UserID,connectionID,storageDT,SPID,isGenotek,Blacklisted from Lab.dna', getengine())\n",
    "    print 'extracted list of  %s FD samples from DB' %len(FDsamples)\n",
    "    \n",
    "    ## generate FD year and date:\n",
    "    FDsamples['storageDT']=FDsamples['storageDT'].astype(str)\n",
    "    FDsamples['FD_Year']=FDsamples['storageDT'].str.split('-').str[0]\n",
    "    FDsamples['FD_Date']=FDsamples['storageDT'].str.split(' ').str[0]\n",
    "    FDsamples['UserID']=FDsamples['UserID'].astype(str)\n",
    "    FDsamples['UserID']=FDsamples['UserID'].str.split('.').str[0]\n",
    "    FDsamples['UserID_fYear']=FDsamples['UserID'].astype(str).str.cat(FDsamples['FD_Year'],sep='_')\n",
    "    FDsamples['UserID_fDate']=FDsamples['UserID'].astype(str).str.cat(FDsamples['FD_Date'],sep='_')\n",
    "    \n",
    "    ### add read number information to FD samples:\n",
    "    #read count info can be taken from either AllSeqProjects (new) or Metabolon2 (old) folder:\n",
    "    if readCountFolder=='Metabolon2':\n",
    "        f1='/net/mraid08/export/jafar/Microbiome/Analyses/Metabolon2/DFOut/ReadCountDF.dat'\n",
    "    else:\n",
    "        f1='/net/mraid08/export/jafar/Microbiome/Analyses/AllSeqProjects/DFOut/ReadCountSpidDF.dat'\n",
    "    ReadCountDF=pd.read_pickle(f1)\n",
    "\n",
    "    #check if there are FD samples repeating twice:\n",
    "    maxRepeatsPerFD=ReadCountDF.reset_index()['FD'].value_counts().sort_values().max()\n",
    "    print 'read count information for %s samples was loaded' %len(ReadCountDF)\n",
    "    print 'maximal repeats per FD samples is %s - SHOULD BE ONE! IF NOT, NEED TO CHECK IT' %maxRepeatsPerFD\n",
    "    #extract only important information to merge with FD table:\n",
    "    ReadCountDFsmall=ReadCountDF.reset_index()[['FD','PostHGF','PostSubSamp']]\n",
    "    #merge read counts info with FD table:\n",
    "    FDsamples_withReads=pd.merge(FDsamples,ReadCountDFsmall,how='left',left_on='DnaID',right_on='FD')\n",
    "    FDsamples_withReads=FDsamples_withReads.drop('FD',axis=1)\n",
    "    #### try to merge unmerged samples based on SPID - CURRENTLY NOT IMPLEMENTED AS IT SEEMS TO GIVE WRONG INFO###\n",
    "#     addInfo=0\n",
    "#     for n in FDsamples_withReads[FDsamples_withReads['PostHGF'].isnull()].index:\n",
    "#         SPID=FDsamples_withReads.loc[n,'SPID']\n",
    "#         if SPID in list(ReadCountDF['SPID']):\n",
    "#             postHGF=ReadCountDF[ReadCountDF['SPID']==SPID].loc[:,'PostHGF']\n",
    "#             PostSubSamp=ReadCountDF[ReadCountDF['SPID']==SPID].loc[:,'PostSubSamp']\n",
    "#             FDsamples_withReads.loc[n,'PostHGF']=postHGF.iloc[0]\n",
    "#             FDsamples_withReads.loc[n,'PostSubSamp']=PostSubSamp.iloc[0]\n",
    "#             addInfo=addInfo+1\n",
    "#     print 'postHGF info was added based on SPID and not FD to %s samples' %addInfo\n",
    "    \n",
    "    \n",
    "    \n",
    "    ## save BD and FD files\n",
    "    print 'saving BD and FD lists to excel files...'\n",
    "    file1='/net/mraid08/export/genie/Lab/Personal/ShaniBAF/Sample files/updatedBDandFDlists/BDfile_%s.xlsx' %cdate\n",
    "    BDwithCorrectUserID.to_excel(file1)\n",
    "\n",
    "    file2='/net/mraid08/export/genie/Lab/Personal/ShaniBAF/Sample files/updatedBDandFDlists/FDfile_RCfolder%s_%s.xlsx' %(readCountFolder,cdate) \n",
    "    FDsamples_withReads.to_excel(file2)\n",
    "    \n",
    "    print 'merging BD and FD samples...'\n",
    "    \n",
    "    BD_FD_converter=pd.merge(BDwithCorrectUserIDandRegs,FDsamples_withReads,how='left',left_on=['UserID','UserID_b%s' %mergeOn],\n",
    "                        right_on=['UserID','UserID_f%s' %mergeOn])\n",
    "    \n",
    "    BD_FD_converter=BD_FD_converter.rename(columns={'DnaID_x':'BD','DnaID_y':'FD'})\n",
    "    \n",
    "    print 'after merging based on UserID and year, there are %s matches and %s BD samples without \\\n",
    "    a match' %(len(BD_FD_converter[BD_FD_converter['FD'].notnull()]),\n",
    "          len(BD_FD_converter[BD_FD_converter['FD'].isnull()]))\n",
    "    \n",
    "    ### merging unmatched BDs to FDs based on UserID only:\n",
    "    ### new strategy started on 16.3.18\n",
    "    \n",
    "    print 'merging unmatched samples based on UserID only...'\n",
    "    BDwithNoFDs=BD_FD_converter[BD_FD_converter['FD'].isnull()]\n",
    "    BDwithFDs=BD_FD_converter[BD_FD_converter['FD'].notnull()]\n",
    "\n",
    "\n",
    "    converterLength=len(BD_FD_converter)\n",
    "    noPostHGF=len(BD_FD_converter[BD_FD_converter['PostHGF'].notnull()])\n",
    "    uniqueBDwithPostHGF=BD_FD_converter[BD_FD_converter['PostHGF'].notnull()]['BD'].nunique()\n",
    "    print 'coverter length is %s' %converterLength\n",
    "    print 'number of rows with noPostHGF info is %s' %noPostHGF\n",
    "    print 'number of unique BDs with postHGF info is %s' %uniqueBDwithPostHGF\n",
    "    \n",
    "    BD_FD_converter['Comment']=''\n",
    "    for BD in list(BDwithNoFDs['BD']):\n",
    "        BDdf=BDwithNoFDs[BDwithNoFDs['BD']==BD]\n",
    "        UserID=list(BDdf.loc[:,'UserID'])[0]\n",
    "        n=BDdf.index\n",
    "        BDtoMerge=BDwithCorrectUserIDandRegs[BDwithCorrectUserIDandRegs['DnaID']==BD]\n",
    "    #     print n.astype(int),BD, UserID\n",
    "    #     print 'length of BDtoMerge is %s and should be 1' %len(BDtoMerge)\n",
    "        correspondingFDdf=FDsamples_withReads[FDsamples_withReads['UserID']==UserID]\n",
    "        if len(correspondingFDdf)>0:\n",
    "            newMerge=pd.merge(BDtoMerge,correspondingFDdf,how='left',\n",
    "                              left_on='UserID',right_on='UserID')\n",
    "            newMerge['Comment']='Not the same year!!'\n",
    "            newMerge=newMerge.rename(columns={'DnaID_x':'BD','DnaID_y':'FD'})\n",
    "    #         print newMerge\n",
    "    #         print 'number of corresponding FD is %s' %len(correspondingFDdf)\n",
    "    #         print 'len of newMerge is %s' %len(newMerge)\n",
    "    #         print 'len of converter before changes is %s' %len(BD_FD_converter)\n",
    "            BD_FD_converter=BD_FD_converter.drop(n,axis=0)\n",
    "            BD_FD_converter=pd.concat([BD_FD_converter,newMerge]) \n",
    "    #         print 'len of converter after changes is %s' %len(BD_FD_converter)\n",
    "    #         print BD_FD_converter[BD_FD_converter['BD']==BD]\n",
    "    \n",
    "    converterLength=len(BD_FD_converter)\n",
    "    noPostHGF=len(BD_FD_converter[BD_FD_converter['PostHGF'].isnull()])\n",
    "    uniqueBDwithPostHGF=BD_FD_converter[BD_FD_converter['PostHGF'].notnull()]['BD'].nunique()\n",
    "    print 'converter length is %s' %converterLength\n",
    "    print 'number of rows with noPostHGF info after new merging is %s' %noPostHGF\n",
    "    print 'number of unique BDs with postHGF info after new merging is %s' %uniqueBDwithPostHGF\n",
    "    \n",
    "    ### remove rows with no FD number:\n",
    "    print 'removing rows with no FD sample:'\n",
    "    final_BD_FD_converter=BD_FD_converter[BD_FD_converter['FD'].notnull()]\n",
    "    print 'n samples after removal=%s' %len(final_BD_FD_converter)\n",
    "    \n",
    "    \n",
    "    \n",
    "    ### save relevant columns only\n",
    "    final_BD_FD_converter=final_BD_FD_converter[['BD', 'Blood_Year', 'Blood_Date','Comment', 'FD', 'FD_Year','FD_Date', u'SPID', u'UserID',\n",
    "                                            'correct registration code','correction status (reg number)','UserID_bYear', 'UserID_fYear',\n",
    "                                             'UserID_bDate', 'UserID_fDate',u'connectionID', u'isGenotek',\n",
    "                                                 'Blacklisted','PostHGF','PostSubSamp']]    \n",
    "    print 'I checked multyBDperFD pairs in file final_BD_FD_converter_mergedOnYear_07032018 (see section 4 below) and saw that trying to resolve those pairs by using dates instead of years was not proved helpful. so i will leave these pairs as is.'\n",
    "    ## add StudyType\n",
    "    #get studyTypeID and UserID information:\n",
    "    studyTypeSPIDs = read_sql('select  * from pnp.spid', getengine())\n",
    "    \n",
    "    studyTypeSPIDs['SPID_Year']=studyTypeSPIDs['Start_date'].astype(str).str.split('-').str[0]\n",
    "    studyTypeSPIDs['UserID']=studyTypeSPIDs['UserID'].astype(str)\n",
    "    studyTypeSPIDs['UserID']=studyTypeSPIDs['UserID'].str.split('.').str[0]\n",
    "    studyTypeSPIDs['UserID_SPIDYear']=studyTypeSPIDs['UserID'].astype(str).str.cat(studyTypeSPIDs['SPID_Year'],sep='_')\n",
    "\n",
    "    studyTypeSPIDs_small=studyTypeSPIDs[['StudyTypeID','UserID_SPIDYear']]\n",
    "    \n",
    "    studyTypelists=studyTypeSPIDs_small.groupby('UserID_SPIDYear').agg(lambda x: x.unique().tolist())\n",
    "    \n",
    "    final_BD_FD_converter=pd.merge(final_BD_FD_converter,studyTypelists,how='left',\n",
    "                                          left_on='UserID_bYear',right_index=True)\n",
    "    \n",
    "    ## add BD index column:\n",
    "    final_BD_FD_converter['BD_index']=final_BD_FD_converter['BD'].str.replace('BD','')\n",
    "    final_BD_FD_converter=final_BD_FD_converter.sort_values(by='BD')\n",
    "    \n",
    "    ## save final file:\n",
    "    print 'final number of samples in the list is %s' %len(final_BD_FD_converter)\n",
    "\n",
    "    print 'saving final files...'\n",
    "    file1='/net/mraid08/export/genie/Lab/Personal/ShaniBAF/Sample files/updatedBDandFDlists/final_BD_FD_converter_mergedOn%s_RCfolder%s_%s.xlsx' %(mergeOn,\n",
    "                                                                                                                                       readCountFolder,cdate)\n",
    "    final_BD_FD_converter.to_excel(file1)\n",
    "\n",
    "    file2='/net/mraid08/export/genie/Lab/Personal/ShaniBAF/Sample files/updatedBDandFDlists/final_BD_FD_converter_mergedOn%s_RCfolder%s_%s' %(mergeOn,\n",
    "                                                                                                                                  readCountFolder,cdate)\n",
    "    final_BD_FD_converter.to_pickle(file2)\n",
    "\n",
    "   \n",
    "    print 'Done!!!'\n",
    "    \n",
    "    return final_BD_FD_converter\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## run fucnction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_BD_FD_converter_YEAR=gen_BD_FD_conversion_file(mergeOn='Year',readCountFolder='Metabolon2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-13T13:56:24.166535Z",
     "start_time": "2018-05-13T13:55:32.589840Z"
    }
   },
   "outputs": [],
   "source": [
    "final_BD_FD_converter_YEAR=gen_BD_FD_conversion_file(mergeOn='Year',readCountFolder='AllSeqProjects')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-31T18:44:49.716538Z",
     "start_time": "2018-05-31T18:43:44.505155Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of rows without BD=35-those rows were removed\n",
      "number of rows after addition of regNums is 1099\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-4:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/wisdom/python/lib/python2.7/threading.py\", line 810, in __bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/usr/wisdom/python/lib/python2.7/threading.py\", line 763, in run\n",
      "    self.__target(*self.__args, **self.__kwargs)\n",
      "  File \"/home/sbenari/workspace/PNP/tunneltoamazondb.py\", line 33, in _start_tunnel\n",
      "    ssh_tunnel.expect(pexpect.EOF)\n",
      "  File \"/usr/wisdom/python/lib/python2.7/site-packages/pexpect/spawnbase.py\", line 321, in expect\n",
      "    timeout, searchwindowsize, async)\n",
      "  File \"/usr/wisdom/python/lib/python2.7/site-packages/pexpect/spawnbase.py\", line 345, in expect_list\n",
      "    return exp.expect_loop(timeout)\n",
      "  File \"/usr/wisdom/python/lib/python2.7/site-packages/pexpect/expect.py\", line 107, in expect_loop\n",
      "    return self.timeout(e)\n",
      "  File \"/usr/wisdom/python/lib/python2.7/site-packages/pexpect/expect.py\", line 70, in timeout\n",
      "    raise TIMEOUT(msg)\n",
      "TIMEOUT: Timeout exceeded.\n",
      "<pexpect.pty_spawn.spawn object at 0x7fb09179a850>\n",
      "command: /usr/bin/ssh\n",
      "args: ['/usr/bin/ssh', '-oStrictHostKeyChecking=no', '-oUserKnownHostsFile=/dev/null', '-N', '-L', '9902:pnpdb.crgdajgv2oja.eu-west-1.rds.amazonaws.com:3306', 'puser@54.228.216.151']\n",
      "buffer (last 100 chars): ' \\r\\n'\n",
      "before (last 100 chars): ' \\r\\n'\n",
      "after: <class 'pexpect.exceptions.TIMEOUT'>\n",
      "match: None\n",
      "match_index: None\n",
      "exitstatus: None\n",
      "flag_eof: False\n",
      "pid: 38046\n",
      "child_fd: 143\n",
      "closed: False\n",
      "timeout: 30\n",
      "delimiter: <class 'pexpect.exceptions.EOF'>\n",
      "logfile: None\n",
      "logfile_read: None\n",
      "logfile_send: None\n",
      "maxread: 2000\n",
      "ignorecase: False\n",
      "searchwindowsize: None\n",
      "delaybeforesend: 0.05\n",
      "delayafterclose: 0.1\n",
      "delayafterterminate: 0.1\n",
      "searcher: searcher_re:\n",
      "    0: EOF\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1100\n",
      "sample number is still 1099\n",
      "extracted list of  2886 FD samples from DB\n",
      "read count information for 6278 samples was loaded\n",
      "maximal repeats per FD samples is 1 - SHOULD BE ONE! IF NOT, NEED TO CHECK IT\n",
      "adding library preparation method:\n",
      "saving BD and FD lists to excel files...\n",
      "merging BD and FD samples...\n",
      "after merging based on UserID and year, there are 1169 matches and 282 BD samples without     a match\n",
      "merging unmatched samples based on UserID only...\n",
      "coverter length is 1451\n",
      "number of rows with noPostHGF info is 1143\n",
      "number of unique BDs with postHGF info is 820\n",
      "converter length is 1880\n",
      "number of rows with noPostHGF info after new merging is 127\n",
      "number of unique BDs with postHGF info after new merging is 1080\n",
      "removing rows with no FD sample:\n",
      "n samples after removal=1848\n",
      "dropping duplicates\n",
      "n samples after removal=1825\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SampleFileFunctions.py:276: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  final_BD_FD_converter['SameYear'] = np.where(final_BD_FD_converter['Blood_Year']==final_BD_FD_converter['FD_Year'],1,0)\n",
      "SampleFileFunctions.py:277: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  final_BD_FD_converter['SameDate'] = np.where(final_BD_FD_converter['Blood_Date']==final_BD_FD_converter['FD_Date'],1,0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " n FDs that repeat severl times within the repeating BD: 19\n",
      "n repeating BDs: 433\n",
      "I checked multyBDperFD pairs in file final_BD_FD_converter_mergedOnYear_07032018 (see section 4 below) and saw that trying to resolve those pairs by using dates instead of years was not proved helpful. so i will leave these pairs as is.\n",
      "final number of samples in the list is 1825\n",
      "saving final files...\n",
      "Done!!!\n"
     ]
    }
   ],
   "source": [
    "final_BD_FD_converter_DATE=gen_BD_FD_conversion_file(mergeOn='Date',readCountFolder='AllSeqProjects')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# develop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## extract blood DNA information:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "BDsamples = read_sql('select  DnaID,UserID,storageDT from Lab.hostdna', getengine())\n",
    "BDsamples=BDsamples.rename(columns={'UserID':'oldUserID'})\n",
    "BDsamples['DnaID_num']=BDsamples['DnaID'].str.replace('BD','').astype(np.float)\n",
    "emptyBDs=BDsamples[BDsamples['DnaID'].isnull()]\n",
    "\n",
    "print 'extracted list of  %s BD samples from DB' %len(BDsamples)\n",
    "print 'last BD index is %s' %BDsamples['DnaID_num'].max()\n",
    "\n",
    "BDsamples=BDsamples[BDsamples['DnaID'].notnull()] #remove rows with no BD\n",
    "print 'number of rows without BD=%s-those rows were removed' %len(emptyBDs)\n",
    "\n",
    "countingBDs=pd.DataFrame(BDsamples['DnaID_num'].value_counts()) #counting repeating BDs\n",
    "repeatingBDs=countingBDs[countingBDs['DnaID_num']>1]\n",
    "BDsamples=BDsamples.drop_duplicates(subset='DnaID') #removing BD duplicates\n",
    "print 'BD samples repeating more than once=%s, duplicates were removed' %list(repeatingBDs.index)\n",
    "\n",
    "\n",
    "print 'final number of rows should be the initial list minus empty BD rows and minus duplicate BD rows\\nand equall to the last BD index'\n",
    "print 'final number of rows is %s' %len(BDsamples)\n",
    "\n",
    "BDsamples.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## correct UserIDs using correct RegNums:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## correct UserIDs using correct RegNums:\n",
    "#open the file with corrected userIDs for each BD number based on Daphna's correction for registration number:\n",
    "file1='/net/mraid08/export/genie/Lab/Personal/ShaniBAF/Sample files/CleanCorrectBloodDNASamples'\n",
    "CleanCorrectBloodDNASamples=pd.read_pickle(file1)\n",
    "BD_correctReg=CleanCorrectBloodDNASamples[['DnaID','original registration code','correct registration code','correction status (reg number)']]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge BD list with correct RegNum list:\n",
    "BDwithCorrectReg=pd.merge(BDsamples,BD_correctReg,how='left',left_on='DnaID',right_on='DnaID')\n",
    "BDwithCorrectReg['correct registration code']=BDwithCorrectReg['correct registration code'].fillna('unknown')\n",
    "BDwithCorrectReg['correction status (reg number)']=BDwithCorrectReg['correction status (reg number)'].fillna('unknown')\n",
    "print 'original BD sample list length is %s' %len(BDsamples)\n",
    "print 'sample list length after merging is %s (SHOULD BE THE SAME AS ABOVE!)' %len(BDwithCorrectReg)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BDwithCorrectReg[BDwithCorrectReg['correction status (reg number)']=='unknown'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# extract UserID-regNum from the database\n",
    "UserIDs_RegNum=read_sql('select UserID,RegistrationCode from pnp.users', getengine())\n",
    "\n",
    "#merge new UserID to BD sample according to the corrected regNum\n",
    "BDwithCorrectUserID=pd.merge(BDwithCorrectReg,UserIDs_RegNum,how='left', \n",
    "                             left_on='correct registration code',\n",
    "                            right_on='RegistrationCode')\n",
    "#give old UserIDs and regNum to samples that don't have correct regNum:\n",
    "noCorrectReg=len(BDwithCorrectUserID[BDwithCorrectUserID['correct registration code']=='unknown'])\n",
    "print '%s samples dont have correct registration code and are given old UserID' %noCorrectReg\n",
    "BDwithCorrectUserID['UserID']=np.where(BDwithCorrectUserID['correct registration code']=='unknown',\n",
    "                                  BDwithCorrectUserID['oldUserID'],BDwithCorrectUserID['UserID'])\n",
    "\n",
    "\n",
    "print 'sample number is still %s' %len(BDwithCorrectUserID)\n",
    "\n",
    "BDwithCorrectUserID=BDwithCorrectUserID[['DnaID','storageDT', 'UserID','correct registration code','correction status (reg number)']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "UserIDs_RegNum.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add regNum according to userID and use it only for samples with unknown regNum:\n",
    "BDwithCorrectUserIDandRegs=pd.merge(BDwithCorrectUserID,UserIDs_RegNum,how='left',\n",
    "                                   left_on='UserID',right_on='UserID')\n",
    "BDwithCorrectUserIDandRegs.head()\n",
    "BDwithCorrectUserIDandRegs['correct registration code']=np.where(BDwithCorrectUserIDandRegs['correct registration code']=='unknown',\n",
    "                                  BDwithCorrectUserIDandRegs['RegistrationCode'],BDwithCorrectUserIDandRegs['correct registration code'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BDwithCorrectUserIDandRegs[BDwithCorrectUserIDandRegs['correction status (reg number)']=='unknown'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BDwithCorrectUserIDandRegs=BDwithCorrectUserIDandRegs.drop('RegistrationCode',axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## generate BD_year and BD_Date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BDwithCorrectUserIDandRegs['storageDT']=BDwithCorrectUserIDandRegs['storageDT'].astype(str)\n",
    "BDwithCorrectUserIDandRegs['Blood_Year']=BDwithCorrectUserIDandRegs['storageDT'].str.split('-').str[0]\n",
    "BDwithCorrectUserIDandRegs['Blood_Date']=BDwithCorrectUserIDandRegs['storageDT'].str.split(' ').str[0]\n",
    "BDwithCorrectUserIDandRegs['UserID']=BDwithCorrectUserIDandRegs['UserID'].astype(str)\n",
    "BDwithCorrectUserIDandRegs['UserID']=BDwithCorrectUserIDandRegs['UserID'].str.split('.').str[0]\n",
    "BDwithCorrectUserIDandRegs['UserID_bYear']=BDwithCorrectUserIDandRegs['UserID'].astype(str).str.cat(BDwithCorrectUserIDandRegs['Blood_Year'],sep='_')\n",
    "BDwithCorrectUserIDandRegs['UserID_bDate']=BDwithCorrectUserIDandRegs['UserID'].astype(str).str.cat(BDwithCorrectUserIDandRegs['Blood_Date'],sep='_')\n",
    "BDwithCorrectUserIDandRegs.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## extract FD table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FDsamples = read_sql('select  DnaID,UserID,connectionID,storageDT,SPID,isGenotek,TubeID from Lab.dna', getengine())\n",
    "print 'extracted list of  %s FD samples from DB' %len(FDsamples)\n",
    "FDsamples.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "FDsamples['isGenotek'].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print len(FDsamples[FDsamples['SPID'].isnull()])\n",
    "print len(FDsamples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## generate FD year and date:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FDsamples['storageDT']=FDsamples['storageDT'].astype(str)\n",
    "FDsamples['FD_Year']=FDsamples['storageDT'].str.split('-').str[0]\n",
    "FDsamples['FD_Date']=FDsamples['storageDT'].str.split(' ').str[0]\n",
    "FDsamples['UserID']=FDsamples['UserID'].astype(str)\n",
    "FDsamples['UserID']=FDsamples['UserID'].str.split('.').str[0]\n",
    "FDsamples['UserID_fYear']=FDsamples['UserID'].astype(str).str.cat(FDsamples['FD_Year'],sep='_')\n",
    "FDsamples['UserID_fDate']=FDsamples['UserID'].astype(str).str.cat(FDsamples['FD_Date'],sep='_')\n",
    "FDsamples.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "FDsamples[FDsamples['UserID']=='2229']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### add read number information to FD samples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "folder='/net/mraid08/export/jafar/Microbiome/Analyses/AllSeqProjects/DFOut'\n",
    "filenames = [f for f in listdir(folder) if isfile(join(folder, f))]\n",
    "filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load read number information for FD samples #####based on ReadCountSpidDF file!!####\n",
    "f1='/net/mraid08/export/jafar/Microbiome/Analyses/AllSeqProjects/DFOut/ReadCountSpidDF.dat'\n",
    "ReadCountDF=pd.read_pickle(f1)\n",
    "print len(ReadCountDF)\n",
    "ReadCountDF.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ReadCountDF.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load read number information for FD samples:\n",
    "# f1='/net/mraid08/export/jafar/Microbiome/Analyses/AllSeqProjects/DFOut/ReadCountDF.dat'\n",
    "# ReadCountDF=pd.read_pickle(f1)\n",
    "# print len(ReadCountDF)\n",
    "# ReadCountDF.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ReadCountDF=ReadCountDF.reset_index()\n",
    "ReadCountDF.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check how many samples have read counts data:\n",
    "print len(ReadCountDF)\n",
    "print ReadCountDF.reset_index()['FD'].nunique()\n",
    "print len(ReadCountDF[ReadCountDF['PostHGF'].notnull()])\n",
    "print ReadCountDF[ReadCountDF['PostHGF'].notnull()].reset_index()['FD'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check if there are FD samples without PostHGF info:\n",
    "ReadCountDF[ReadCountDF['PostHGF'].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check if there are FD samples repeating twice:\n",
    "maxRepeatsPerFD=ReadCountDF.reset_index()['FD'].value_counts().sort_values().max()\n",
    "print 'read count information for %s samples was loaded' %len(ReadCountDF)\n",
    "print 'maximal repeats per FD samples is %s - SHOULD BE ONE! IF NOT, NEED TO CHECK IT' %maxRepeatsPerFD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "no FD repeats more than once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract only important information to merge with FD table:\n",
    "ReadCountDFsmall=ReadCountDF.reset_index()[['FD','PostHGF','PostSubSamp']]\n",
    "print len(ReadCountDFsmall[ReadCountDFsmall['PostHGF'].isnull()])\n",
    "ReadCountDFsmall.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'FD238' in list(ReadCountDFsmall['FD'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#merge read counts info with FD table:\n",
    "FDsamples_withReads=pd.merge(FDsamples,ReadCountDFsmall,how='left',left_on='DnaID',right_on='FD')\n",
    "FDsamples_withReads=FDsamples_withReads.drop('FD',axis=1)\n",
    "print len(FDsamples)\n",
    "print len(FDsamples_withReads)\n",
    "FDsamples_withReads.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FDsamples_withReads[FDsamples_withReads['DnaID']=='FD238']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### try to merge unmerged samples based on SPID:\n",
    "I could add read count info to 168 additional FD samples, but it seems problematic - it gives read counts of DIFFERENT FD sample from the same SPID.\n",
    "not implemented at the moment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# addInfo=0\n",
    "# for n in FDsamples_withReads[FDsamples_withReads['PostHGF'].isnull()].index:\n",
    "# #     print n\n",
    "#     SPID=FDsamples_withReads.loc[n,'SPID']\n",
    "#     FDinFDtable=FDsamples_withReads.loc[n,'DnaID']\n",
    "    \n",
    "#     if SPID in list(ReadCountDF['SPID']):\n",
    "#         postHGF=ReadCountDF[ReadCountDF['SPID']==SPID].loc[:,'PostHGF']\n",
    "#         PostSubSamp=ReadCountDF[ReadCountDF['SPID']==SPID].loc[:,'PostSubSamp']\n",
    "#         FDinReadCountDF=ReadCountDF[ReadCountDF['SPID']==SPID].loc[:,'FD']\n",
    "#         FDsamples_withReads.loc[n,'PostHGF']=postHGF.iloc[0]\n",
    "#         FDsamples_withReads.loc[n,'PostSubSamp']=PostSubSamp.iloc[0]\n",
    "# #         print n,postHGF.iloc[0],PostSubSamp.iloc[0]\n",
    "#         addInfo=addInfo+1\n",
    "#         print 'info added to SPID %s, FDinFDtable is %s and  FDinReadCountDF is %s' %(SPID,FDinFDtable,FDinReadCountDF)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "addInfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check how many samples have read counts data:\n",
    "print len(FDsamples_withReads)\n",
    "print FDsamples_withReads['DnaID'].nunique()\n",
    "print len(FDsamples_withReads[FDsamples_withReads['PostHGF'].notnull()])\n",
    "print FDsamples_withReads[FDsamples_withReads['PostHGF'].notnull()]['DnaID'].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### add meeting location and sequencing machine information to FD samples:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOT READY! DO NOT USE YET!!!!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1='/net/mraid08/export/jafar/Microbiome/Analyses/AllSeqProjects/DFOut/ReadCountDF.dat'\n",
    "StoolMetadataDF=pd.read_pickle(f1)\n",
    "print len(StoolMetadataDF)\n",
    "StoolMetadataDF.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1='/net/mraid08/export/jafar/Microbiome/Analyses/AllSeqProjects/DFOut/ReadCountSpidDF.dat'\n",
    "ReadCountDF=pd.read_pickle(f1)\n",
    "print len(ReadCountDF)\n",
    "ReadCountDF.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "StoolMetadataDF.reset_index()['FD'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FDinConverter=list(final_BD_FD_converter_YEAR['FD'].unique())\n",
    "FDinNewFile=list(StoolMetadataDF.reset_index()['FD'].unique())\n",
    "FDinReadCount=list(ReadCountDF.reset_index()['FD'].unique())\n",
    "FDinFDsamples_withReads=list(FDsamples_withReads.reset_index()['DnaID'].unique())\n",
    "print len(FDinConverter)\n",
    "print len(FDinNewFile)\n",
    "print len(FDinReadCount)\n",
    "print len(FDinFDsamples_withReads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'FD238' in FDinFDsamples_withReads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'FD238' in FDinReadCount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'FD238' in FDinNewFile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ReadCountDF=ReadCountDF.reset_index()\n",
    "ReadCountDF[ReadCountDF['FD']=='FD238']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ReadCount_NewFile=list(set(FDinReadCount+FDinNewFile))\n",
    "print len(ReadCount_NewFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=FDinConverter\n",
    "b=FDinNewFile\n",
    "print len(set(a).intersection(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1='/net/mraid08/export/jafar/Microbiome/Analyses/Metabolon2/DFOut/MeasurementsDF.dat'\n",
    "MeasurementsDF=pd.read_pickle(f1)\n",
    "print len(MeasurementsDF)\n",
    "MeasurementsDF.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FDsamples_withReads['TubeIDindex']=FDsamples_withReads['TubeID'].str.replace('fid','')\n",
    "FDsamples_withReads.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# extract meeting location informaiton for stool sample from Lab.stool:\n",
    "StoolSamples = read_sql('select UserID, ConnectionID, ProductionDT,storageDT,FID1,FID2,BoxLocation,SPID from Lab.stool', getengine())\n",
    "StoolSamples['TubeIDindex1']=StoolSamples['FID1'].str.replace('fid','',case=False)\n",
    "StoolSamples['TubeIDindex2']=StoolSamples['FID2'].str.replace('fid','',case=False)\n",
    "print len(StoolSamples)\n",
    "StoolSamples.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract year and date:\n",
    "StoolSamples['storageDT']=StoolSamples['storageDT'].astype(str)\n",
    "StoolSamples['stool_Year']=StoolSamples['storageDT'].str.split('-').str[0]\n",
    "StoolSamples['stool_Date']=StoolSamples['storageDT'].str.split(' ').str[0]\n",
    "StoolSamples['UserID']=StoolSamples['UserID'].astype(str)\n",
    "StoolSamples['UserID']=StoolSamples['UserID'].str.split('.').str[0]\n",
    "StoolSamples['UserID_sYear']=StoolSamples['UserID'].astype(str).str.cat(StoolSamples['stool_Year'],sep='_')\n",
    "StoolSamples['UserID_sDate']=StoolSamples['UserID'].astype(str).str.cat(StoolSamples['stool_Date'],sep='_')\n",
    "StoolSamples.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#merge stool and FD samples on user+year:\n",
    "FDsamples_withReads_withLocation=pd.merge(FDsamples_withReads,StoolSamples,how='left',\n",
    "                                         left_on='UserID_fDate',right_on='UserID_sDate')\n",
    "FDsamples_withReads_withLocation.head()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "print len(FDsamples_withReads)\n",
    "print len(StoolSamples)\n",
    "print len(FDsamples_withReads_withLocation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOT GOOD TO MERGE ON DATE/YEAR+USER - TOO MANY UNCERTAINTIES!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## save BD and FD files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print 'saving BD and FD lists to excel files...'\n",
    "file1='/net/mraid08/export/genie/Lab/Personal/ShaniBAF/Sample files/updatedBDandFDlists/BDfile_%s.xlsx' %cdate\n",
    "BDwithCorrectUserID.to_excel(file1)\n",
    "\n",
    "file2='/net/mraid08/export/genie/Lab/Personal/ShaniBAF/Sample files/updatedBDandFDlists/FDfile_%s.xlsx' %cdate\n",
    "FDsamples_withReads.to_excel(file2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MERGE FD and BD tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print 'merging BD and FD samples...'\n",
    "BD_FD_converter=pd.merge(BDwithCorrectUserIDandRegs,FDsamples_withReads,how='left',left_on=['UserID','UserID_bYear'],\n",
    "                        right_on=['UserID','UserID_fYear'])\n",
    "BD_FD_converter.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BD_FD_converter.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BD_FD_converter=BD_FD_converter.rename(columns={'DnaID_x':'BD','DnaID_y':'FD'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BD_FD_converter.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print 'after merging based on UserID and year, there are %s matches and %s BD samples without \\\n",
    "# a match' %(len(BD_FD_converter[BD_FD_converter['FD'].notnull()]),\n",
    "#           len(BD_FD_converter[BD_FD_converter['FD'].isnull()]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print 'after merging based on UserID and Date, there are %s matches and %s BD samples without \\\n",
    "a match' %(len(BD_FD_converter[BD_FD_converter['FD'].notnull()]),\n",
    "          len(BD_FD_converter[BD_FD_converter['FD'].isnull()]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BD_FD_converter[BD_FD_converter['FD'].isnull()].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BD_FD_converter[BD_FD_converter['BD']=='BD345']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### merging unmatched BDs to FDs based on UserID only:\n",
    "new strategy started on 16.3.18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BDwithNoFDs=BD_FD_converter[BD_FD_converter['FD'].isnull()]\n",
    "BDwithFDs=BD_FD_converter[BD_FD_converter['FD'].notnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BDwithNoFDs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BDwithNoFDs.index.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print len(BDwithNoFDs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BDwithNoFDs['BD'].value_counts().max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "converterLength=len(BD_FD_converter)\n",
    "noPostHGF=len(BD_FD_converter[BD_FD_converter['PostHGF'].notnull()])\n",
    "uniqueBDwithPostHGF=BD_FD_converter[BD_FD_converter['PostHGF'].notnull()]['BD'].nunique()\n",
    "print 'coverter length is %s' %converterLength\n",
    "print 'number of rows with noPostHGF info is %s' %noPostHGF\n",
    "print 'number of unique BDs with postHGF info is %s' %uniqueBDwithPostHGF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BD_FD_converter['Comment']=''\n",
    "for BD in list(BDwithNoFDs['BD']):\n",
    "    BDdf=BDwithNoFDs[BDwithNoFDs['BD']==BD]\n",
    "    UserID=list(BDdf.loc[:,'UserID'])[0]\n",
    "    n=BDdf.index\n",
    "    BDtoMerge=BDwithCorrectUserIDandRegs[BDwithCorrectUserIDandRegs['DnaID']==BD]\n",
    "#     print n.astype(int),BD, UserID\n",
    "#     print 'length of BDtoMerge is %s and should be 1' %len(BDtoMerge)\n",
    "    correspondingFDdf=FDsamples_withReads[FDsamples_withReads['UserID']==UserID]\n",
    "    if len(correspondingFDdf)>0:\n",
    "        newMerge=pd.merge(BDtoMerge,correspondingFDdf,how='left',\n",
    "                          left_on='UserID',right_on='UserID')\n",
    "        newMerge['Comment']='Not the same year!!'\n",
    "        newMerge=newMerge.rename(columns={'DnaID_x':'BD','DnaID_y':'FD'})\n",
    "#         print newMerge\n",
    "#         print 'number of corresponding FD is %s' %len(correspondingFDdf)\n",
    "#         print 'len of newMerge is %s' %len(newMerge)\n",
    "#         print 'len of converter before changes is %s' %len(BD_FD_converter)\n",
    "#         BD_FD_converter=BD_FD_converter.drop(n,axis=0)\n",
    "        BD_FD_converter=pd.concat([BD_FD_converter,newMerge]) \n",
    "#         print 'len of converter after changes is %s' %len(BD_FD_converter)\n",
    "#         print BD_FD_converter[BD_FD_converter['BD']==BD]\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "converterLength=len(BD_FD_converter)\n",
    "noPostHGF=len(BD_FD_converter[BD_FD_converter['PostHGF'].isnull()])\n",
    "uniqueBDwithPostHGF=BD_FD_converter[BD_FD_converter['PostHGF'].notnull()]['BD'].nunique()\n",
    "print 'converter length is %s' %converterLength\n",
    "print 'number of rows with noPostHGF info after new merging is %s' %noPostHGF\n",
    "print 'number of unique BDs with postHGF info after new merging is %s' %uniqueBDwithPostHGF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BD_FD_converter[BD_FD_converter['BD']=='BD345']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # go over BD samples without match FD and match FD based on userID only\n",
    "# BDwithNoFDs=BD_FD_converter[BD_FD_converter['FD'].isnull()]\n",
    "# BDwithFDs=BD_FD_converter[BD_FD_converter['FD'].notnull()]\n",
    "\n",
    "\n",
    "# BD_FD_converter['Comment']=''\n",
    "# for n in BD_FD_converter[BD_FD_converter['FD'].isnull()].index:\n",
    "#         print 'n=%s' %n\n",
    "#         userID=BD_FD_converter.loc[n,'UserID']\n",
    "#         relevantDF=FDsamples[FDsamples['UserID']==userID] #generate a df with all FD samples for the specific userID\n",
    "#         relevantDF=relevantDF.set_index('DnaID')\n",
    "#         print len(relevantDF)\n",
    "#         DFlength=len(relevantDF)\n",
    "#         FDlist=list(relevantDF.index)\n",
    "#         relevantFDlist=[x for x in FDlist if x not in list(BDwithFDs['FD'])] #use only FDs that belong to the UserID and ARE NOT\n",
    "#                                                                      # connected already with another BD\n",
    "#         relevantDF2=relevantDF.loc[relevantFDlist,:]\n",
    "#         relevantDF2=relevantDF2.reset_index()\n",
    "#         print len(relevantDF2)\n",
    "#         FDlist2=list(relevantDF2['DnaID'])\n",
    "#         UserFlist=list(relevantDF2['UserID'])\n",
    "#         FDyearList=list(relevantDF2['FD_Year'])\n",
    "#         UserID_fYearList=list(relevantDF2['UserID_fYear'])\n",
    "#         FDDateList=list(relevantDF2['FD_Date'])\n",
    "#         UserID_fDateList=list(relevantDF2['UserID_fDate'])\n",
    "#         BDlist=BD_FD_converter.loc[n,'BD']\n",
    "#         Blood_Yearlist=BD_FD_converter.loc[n,'Blood_Year']\n",
    "#         UserID_bYearlist=BD_FD_converter.loc[n,'UserID_bYear']\n",
    "#         Blood_Datelist=BD_FD_converter.loc[n,'Blood_Date']\n",
    "#         UserID_bDatelist=BD_FD_converter.loc[n,'UserID_bDate']\n",
    "#         correctRegList=BD_FD_converter.loc[n,'correct registration code']\n",
    "#         correctStatusList=BD_FD_converter.loc[n,'correction status (reg number)']\n",
    "#         newDF=pd.DataFrame({'BD':BDlist,\n",
    "#                             'UserID':userID,\n",
    "#                             'FD':FDlist2,\n",
    "#                             'FD_Year':FDyearList,\n",
    "#                             'UserID_fYear':UserID_fYearList,\n",
    "#                             'FD_Date':FDDateList,\n",
    "#                             'UserID_fDate':UserID_fDateList,\n",
    "#                             'Blood_Year':Blood_Yearlist,\n",
    "#                             'UserID_bYear':UserID_bYearlist,\n",
    "#                             'Blood_Date':Blood_Datelist,\n",
    "#                             'UserID_bDate':UserID_bDatelist,\n",
    "#                             'correct registration code':correctRegList,\n",
    "#                             'correction status (reg number)':correctStatusList,\n",
    "#                             'Comment':'Not the same year!!'})\n",
    "        \n",
    "# #         print newDF\n",
    "# #         print BD_FD_converter.columns.values\n",
    "#         BD_FD_converter=pd.concat([BD_FD_converter,newDF])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BD_FD_converter.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # remove repeating FD samples:\n",
    "# BD_FD_converter=BD_FD_converter.reset_index()\n",
    "# print 'removing repeating FD samples:'\n",
    "# print 'n samples before removal: %s' %len(BD_FD_converter)\n",
    "# groupbyFD=BD_FD_converter.groupby('FD')['BD'].nunique()\n",
    "# moreThan1=groupbyFD[groupbyFD>1]\n",
    "# moreThan1\n",
    "\n",
    "# BD_FD_converter2=BD_FD_converter\n",
    "# for FD in moreThan1.index:\n",
    "#     df=BD_FD_converter[BD_FD_converter['FD']==FD]\n",
    "# #     print FD, len(df),len(df[df['Comment']==''])\n",
    "#     if len(df)>1 and len(df[df['Comment']==''])>0:\n",
    "# #         print df[df['Comment']=='Not the same year!!']\n",
    "#         for n in df[df['Comment']=='Not the same year!!'].index:\n",
    "#             print 'dropping the following row:'\n",
    "#             print BD_FD_converter2.loc[n,:]\n",
    "#             BD_FD_converter2=BD_FD_converter2.drop(n,axis=0)\n",
    "            \n",
    "        \n",
    "# print 'n samples after removal: %s' %len(BD_FD_converter2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_BD_FD_converter2=BD_FD_converter[BD_FD_converter['FD'].notnull()]\n",
    "final_BD_FD_converter2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print len(BD_FD_converter)\n",
    "print len(final_BD_FD_converter2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print BD_FD_converter['BD'].nunique()\n",
    "print final_BD_FD_converter2['BD'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_BD_FD_converter=final_BD_FD_converter2[['BD', 'Blood_Year', 'Blood_Date','Comment', 'FD', 'FD_Year','FD_Date', u'SPID', u'UserID',\n",
    "                                            'correct registration code','correction status (reg number)','UserID_bYear', 'UserID_fYear',\n",
    "                                             'UserID_bDate', 'UserID_fDate',u'connectionID', u'isGenotek',\n",
    "                                             'PostHGF','PostSubSamp']]\n",
    "\n",
    "final_BD_FD_converter.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### work on FD-BD pairs from the same year:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I checked those pairs in file final_BD_FD_converter_mergedOnYear_07032018 (see section 4 below) and saw that tring to resolve those pairs by using dates instead of years is not proved helpful. so i'll leave this pairs as is. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(final_BD_FD_converter.groupby('FD')['BD'].nunique()).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "countBDperFD=pd.DataFrame(final_BD_FD_converter.groupby('FD')['BD'].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multyBDperFD=countBDperFD[countBDperFD['BD']>1]\n",
    "print len(multyBDperFD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for FD in multyBDperFD.index:\n",
    "    df=final_BD_FD_converter[final_BD_FD_converter['FD']==FD]\n",
    "    print FD\n",
    "    print df\n",
    "#     for n in df.index:\n",
    "#         BD=df.loc[n,'BD']\n",
    "#         Blood_Date=df.loc[n,'Blood_Date']\n",
    "#         FD_Date=df.loc[n,'FD']\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_BD_FD_converter['correction status (reg number)'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print len(final_BD_FD_converter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## add StudyType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#get studyTypeID and UserID information:\n",
    "studyTypeSPIDs = read_sql('select  * from pnp.spid', getengine())\n",
    "studyTypeSPIDs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "studyTypeSPIDs['SPID_Year']=studyTypeSPIDs['Start_date'].astype(str).str.split('-').str[0]\n",
    "studyTypeSPIDs['SPID_Date']=studyTypeSPIDs['Start_date'].astype(str).str.split(' ').str[0]\n",
    "studyTypeSPIDs['UserID']=studyTypeSPIDs['UserID'].astype(str)\n",
    "studyTypeSPIDs['UserID']=studyTypeSPIDs['UserID'].str.split('.').str[0]\n",
    "studyTypeSPIDs['UserID_SPIDYear']=studyTypeSPIDs['UserID'].astype(str).str.cat(studyTypeSPIDs['SPID_Year'],sep='_')\n",
    "studyTypeSPIDs['UserID_SPIDDate']=studyTypeSPIDs['UserID'].astype(str).str.cat(studyTypeSPIDs['SPID_Date'],sep='_')\n",
    "studyTypeSPIDs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "studyTypeSPIDs_small=studyTypeSPIDs[['StudyTypeID','UserID_SPIDYear']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "studyTypelists=studyTypeSPIDs_small.groupby('UserID_SPIDYear').agg(lambda x: x.unique().tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "studyTypelists[200:205]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_BD_FD_converter=pd.merge(final_BD_FD_converter,studyTypelists,how='left',\n",
    "                                          left_on='UserID_bYear',right_index=True)\n",
    "final_BD_FD_converter.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## add postHGF info to FD samples based on registrant numbers:\n",
    "it seems that registrant numbers in the read count table are not similar to regNum in the converter file, so I dropped this direction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## save final file:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print 'final number of samples in the list is %s' %len(final_BD_FD_converter)\n",
    "\n",
    "print 'saving final files...'\n",
    "file1='/net/mraid08/export/genie/Lab/Personal/ShaniBAF/Sample files/updatedBDandFDlists/final_BD_FD_converter_%s.xlsx' %cdate \n",
    "final_BD_FD_converter.to_excel(file1)\n",
    "\n",
    "file2='/net/mraid08/export/genie/Lab/Personal/ShaniBAF/Sample files/updatedBDandFDlists/final_BD_FD_converter_%s' %cdate\n",
    "final_BD_FD_converter.to_pickle(file2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_BD_FD_converter.groupby('FD')['BD'].nunique().sort_values(ascending=False)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BDperFD=pd.DataFrame(final_BD_FD_converter.groupby('FD')['BD'].nunique().sort_values(ascending=False))\n",
    "BDperFD.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print len(BDperFD[BDperFD['BD']>1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# compare converter file quality:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "file2='/net/mraid08/export/genie/Lab/Personal/ShaniBAF/Sample files/updatedBDandFDlists/final_BD_FD_converter' \n",
    "final_BD_FD_converter=pd.read_pickle(file2)\n",
    "print len(final_BD_FD_converter)\n",
    "final_BD_FD_converter.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "file2='/net/mraid08/export/genie/Lab/Personal/ShaniBAF/Sample files/updatedBDandFDlists/final_BD_FD_converter_mergedOnYear_07032018' \n",
    "final_BD_FD_converter_mergedOnYear_07032018=pd.read_pickle(file2)\n",
    "print len(final_BD_FD_converter_mergedOnYear_07032018)\n",
    "final_BD_FD_converter_mergedOnYear_07032018.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "file2='/net/mraid08/export/genie/Lab/Personal/ShaniBAF/Sample files/updatedBDandFDlists/final_BD_FD_converter_mergedOnDate_07032018' \n",
    "final_BD_FD_converter_mergedOnDate_07032018=pd.read_pickle(file2)\n",
    "print len(final_BD_FD_converter_mergedOnDate_07032018)\n",
    "final_BD_FD_converter_mergedOnDate_07032018.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "file2='/net/mraid08/export/genie/Lab/Personal/ShaniBAF/Sample files/updatedBDandFDlists/final_BD_FD_converter_mergedOnYear_14032018' \n",
    "final_BD_FD_converter_mergedOnYear_14032018=pd.read_pickle(file2)\n",
    "print len(final_BD_FD_converter_mergedOnYear_14032018)\n",
    "final_BD_FD_converter_mergedOnYear_14032018.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print final_BD_FD_converter['BD'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print final_BD_FD_converter_mergedOnYear_07032018['BD'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print final_BD_FD_converter_mergedOnYear_14032018['BD'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_BD_FD_converter[final_BD_FD_converter['UserID'].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_BD_FD_converter_mergedOnYear_07032018[final_BD_FD_converter_mergedOnYear_07032018['UserID'].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_BD_FD_converter_mergedOnYear_14032018[final_BD_FD_converter_mergedOnYear_14032018['UserID'].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print len(final_BD_FD_converter[final_BD_FD_converter['SPID'].isnull()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print len(final_BD_FD_converter_mergedOnYear_07032018[final_BD_FD_converter_mergedOnYear_07032018['SPID'].isnull()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print len(final_BD_FD_converter_mergedOnYear_14032018[final_BD_FD_converter_mergedOnYear_14032018['SPID'].isnull()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_BD_FD_converter[final_BD_FD_converter['FD'].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print len(final_BD_FD_converter[final_BD_FD_converter['connectionID'].isnull()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print len(final_BD_FD_converter_mergedOnYear_07032018[final_BD_FD_converter_mergedOnYear_07032018['connectionID'].isnull()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print len(final_BD_FD_converter_mergedOnYear_14032018[final_BD_FD_converter_mergedOnYear_14032018['connectionID'].isnull()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "countBDperFD_07032018=pd.DataFrame(final_BD_FD_converter_mergedOnYear_07032018.groupby('FD')['BD'].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multyBDperFD_07032018=countBDperFD_07032018[countBDperFD_07032018['BD']>1]\n",
    "print len(multyBDperFD_07032018)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "countBDperFD_14032018=pd.DataFrame(final_BD_FD_converter_mergedOnYear_14032018.groupby('FD')['BD'].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multyBDperFD_14032018=countBDperFD_14032018[countBDperFD_14032018['BD']>1]\n",
    "print len(multyBDperFD_14032018)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for FD in multyBDperFD_07032018.index:\n",
    "    df=final_BD_FD_converter_mergedOnYear_07032018[final_BD_FD_converter_mergedOnYear_07032018['FD']==FD]\n",
    "    print FD\n",
    "    print df\n",
    "#     for n in df.index:\n",
    "#         BD=df.loc[n,'BD']\n",
    "#         Blood_Date=df.loc[n,'Blood_Date']\n",
    "#         FD_Date=df.loc[n,'FD']\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for FD in multyBDperFD_14032018.index:\n",
    "    df=final_BD_FD_converter_mergedOnYear_14032018[final_BD_FD_converter_mergedOnYear_14032018['FD']==FD]\n",
    "    print FD\n",
    "    print df\n",
    "#     for n in df.index:\n",
    "#         BD=df.loc[n,'BD']\n",
    "#         Blood_Date=df.loc[n,'Blood_Date']\n",
    "#         FD_Date=df.loc[n,'FD']\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "countBDperFD_07032018_date=pd.DataFrame(final_BD_FD_converter_mergedOnDate_07032018.groupby('FD')['BD'].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "countBDperFD_14032018_date=pd.DataFrame(final_BD_FD_converter_mergedOnDate_14032018.groupby('FD')['BD'].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multyBDperFD_07032018_date=countBDperFD_07032018_date[countBDperFD_07032018_date['BD']>1]\n",
    "print len(multyBDperFD_07032018_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multyBDperFD_14032018_date=countBDperFD_14032018_date[countBDperFD_14032018_date['BD']>1]\n",
    "print len(multyBDperFD_14032018_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for FD in multyBDperFD_07032018_date.index:\n",
    "    df=final_BD_FD_converter_mergedOnDate_07032018[final_BD_FD_converter_mergedOnDate_07032018['FD']==FD]\n",
    "    print FD\n",
    "    print df\n",
    "#     for n in df.index:\n",
    "#         BD=df.loc[n,'BD']\n",
    "#         Blood_Date=df.loc[n,'Blood_Date']\n",
    "#         FD_Date=df.loc[n,'FD']\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### explore missing identifiers in the mergeOn=Date file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_BD_FD_converter_mergedOnYear_07032018.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noDate=final_BD_FD_converter_mergedOnYear_07032018[final_BD_FD_converter_mergedOnYear_07032018['Blood_Date'].isnull()]\n",
    "print len(noDate)\n",
    "noDate.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noFD=final_BD_FD_converter_mergedOnYear_07032018[final_BD_FD_converter_mergedOnYear_07032018['FD'].isnull()]\n",
    "print len(noFD)\n",
    "noFD.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noSPID=final_BD_FD_converter_mergedOnYear_07032018[final_BD_FD_converter_mergedOnYear_07032018['SPID'].isnull()]\n",
    "print len(noSPID)\n",
    "noSPID.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noConnection=final_BD_FD_converter_mergedOnYear_07032018[final_BD_FD_converter_mergedOnYear_07032018['connectionID'].isnull()]\n",
    "print len(noConnection)\n",
    "noConnection.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# add library preparation method to FD samples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-30T11:18:31.921165Z",
     "start_time": "2018-04-30T11:18:31.889200Z"
    }
   },
   "outputs": [],
   "source": [
    "f1='%s/Sample files/all_fds_barcode_df.dat' %MyPath\n",
    "libPrep=pd.read_pickle(f1)\n",
    "libPrep.Barcode.value_counts()\n",
    "# libPrep.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-30T09:35:11.270136Z",
     "start_time": "2018-04-30T09:35:11.263008Z"
    }
   },
   "outputs": [],
   "source": [
    "libPrep.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# some checks - 130518"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-13T10:06:58.346235Z",
     "start_time": "2018-05-13T10:06:58.328199Z"
    }
   },
   "outputs": [],
   "source": [
    "file1 = '/net/mraid08/export/genie/Lab/Personal/ShaniBAF/Sample files/CleanCorrectBloodDNASamples'\n",
    "CleanCorrectBloodDNASamples = pd.read_pickle(file1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-13T10:06:58.597493Z",
     "start_time": "2018-05-13T10:06:58.520310Z"
    }
   },
   "outputs": [],
   "source": [
    "CleanCorrectBloodDNASamples.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-13T10:07:00.502387Z",
     "start_time": "2018-05-13T10:07:00.484405Z"
    }
   },
   "outputs": [],
   "source": [
    "CleanCorrectBloodDNASamples[CleanCorrectBloodDNASamples['DnaID']=='BD38']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-13T10:07:07.516600Z",
     "start_time": "2018-05-13T10:07:01.378618Z"
    }
   },
   "outputs": [],
   "source": [
    "UserIDs_RegNum = read_sql('select UserID,RegistrationCode from pnp.users', getengine())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-13T10:07:08.567654Z",
     "start_time": "2018-05-13T10:07:08.553833Z"
    }
   },
   "outputs": [],
   "source": [
    "UserIDs_RegNum.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-13T10:07:09.524134Z",
     "start_time": "2018-05-13T10:07:09.513164Z"
    }
   },
   "outputs": [],
   "source": [
    "UserIDs_RegNum[UserIDs_RegNum['RegistrationCode']==541391]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-13T10:07:12.269172Z",
     "start_time": "2018-05-13T10:07:10.485104Z"
    }
   },
   "outputs": [],
   "source": [
    "BDsamples = read_sql('select  DnaID,UserID,storageDT from Lab.hostdna', getengine())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-13T10:07:13.337453Z",
     "start_time": "2018-05-13T10:07:13.323130Z"
    }
   },
   "outputs": [],
   "source": [
    "BDsamples.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-13T10:07:14.393974Z",
     "start_time": "2018-05-13T10:07:14.378694Z"
    }
   },
   "outputs": [],
   "source": [
    "BDsamples[BDsamples['DnaID']=='BD38']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-13T10:07:15.491854Z",
     "start_time": "2018-05-13T10:07:15.482678Z"
    }
   },
   "outputs": [],
   "source": [
    "BDsamples[BDsamples['UserID']==11343]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-13T12:19:47.003075Z",
     "start_time": "2018-05-13T12:19:40.914937Z"
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "cdate = str(time.strftime(\"%d%m%Y\"))\n",
    "cdate\n",
    "\n",
    "# # extract blood DNA information:\n",
    "BDsamples = read_sql('select  DnaID,UserID,storageDT from Lab.hostdna', getengine())\n",
    "BDsamples = BDsamples.rename(columns={'UserID':'oldUserID'})\n",
    "BDsamples['DnaID_num'] = BDsamples['DnaID'].str.replace('BD', '').astype(np.float)\n",
    "emptyBDs = BDsamples[BDsamples['DnaID'].isnull()]\n",
    "BDsamples = BDsamples[BDsamples['DnaID'].notnull()] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-13T12:19:48.159767Z",
     "start_time": "2018-05-13T12:19:48.143517Z"
    }
   },
   "outputs": [],
   "source": [
    "print BDsamples[BDsamples['DnaID']=='BD937']\n",
    "print BDsamples[BDsamples['DnaID']=='BD938']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-13T12:19:49.938769Z",
     "start_time": "2018-05-13T12:19:49.923967Z"
    }
   },
   "outputs": [],
   "source": [
    "print 'number of rows without BD=%s-those rows were removed' % len(emptyBDs)\n",
    "\n",
    "countingBDs = pd.DataFrame(BDsamples['DnaID_num'].value_counts())  # counting repeating BDs\n",
    "repeatingBDs = countingBDs[countingBDs['DnaID_num'] > 1]\n",
    "print repeatingBDs\n",
    "BDsamples = BDsamples.drop_duplicates(subset='DnaID')  # removing BD duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-13T12:19:51.533674Z",
     "start_time": "2018-05-13T12:19:51.517124Z"
    }
   },
   "outputs": [],
   "source": [
    "print BDsamples[BDsamples['DnaID']=='BD937']\n",
    "print BDsamples[BDsamples['DnaID']=='BD938']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-13T12:23:32.003094Z",
     "start_time": "2018-05-13T12:23:25.869261Z"
    }
   },
   "outputs": [],
   "source": [
    "UserIDs_RegNum = read_sql('select UserID,RegistrationCode from pnp.users', getengine())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-13T12:29:52.381842Z",
     "start_time": "2018-05-13T12:29:52.352818Z"
    }
   },
   "outputs": [],
   "source": [
    "# merge regNums to BD samples:\n",
    "BDsamples_withOldIDs=pd.merge(BDsamples,UserIDs_RegNum,how='left',left_on='oldUserID',right_on='UserID')\n",
    "BDsamples_withOldIDs=BDsamples_withOldIDs.drop('UserID', axis=1)\n",
    "BDsamples_withOldIDs=BDsamples_withOldIDs.rename(columns={'RegistrationCode':'old_RegistrationCode'})\n",
    "print len(BDsamples_withOldIDs)\n",
    "BDsamples_withOldIDs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-13T12:30:47.086572Z",
     "start_time": "2018-05-13T12:30:40.056279Z"
    }
   },
   "outputs": [],
   "source": [
    "from ShaniBA.SampleLists.correctingRegNumsFromDaphna import getQCedBloodRegNumByRegNum\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-13T12:33:47.450236Z",
     "start_time": "2018-05-13T12:33:14.101955Z"
    }
   },
   "outputs": [],
   "source": [
    "BDsamples_corrected=BDsamples_withOldIDs\n",
    "\n",
    "BDsamples_corrected['corrected_regCode']=BDsamples_corrected['old_RegistrationCode'].apply(lambda x:getQCedBloodRegNumByRegNum(x))\n",
    "BDsamples_corrected.head(10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-13T12:34:46.280957Z",
     "start_time": "2018-05-13T12:34:46.275565Z"
    }
   },
   "outputs": [],
   "source": [
    "print len(BDsamples_corrected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-13T12:35:11.208874Z",
     "start_time": "2018-05-13T12:35:11.194064Z"
    }
   },
   "outputs": [],
   "source": [
    "print BDsamples_corrected[BDsamples_corrected['corrected_regCode']!=BDsamples_corrected['old_RegistrationCode']].head()\n",
    "print len(BDsamples_corrected[BDsamples_corrected['corrected_regCode']!=BDsamples_corrected['old_RegistrationCode']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-13T12:40:11.211508Z",
     "start_time": "2018-05-13T12:40:11.164000Z"
    }
   },
   "outputs": [],
   "source": [
    "BDsamples_corrected['correction status (reg number)']=np.where(BDsamples_corrected['corrected_regCode']!=BDsamples_corrected['old_RegistrationCode'],'corrected','same')\n",
    "BDsamples_corrected['correction status (reg number)']=np.where(BDsamples_corrected['corrected_regCode'].isnull(),'unknown',BDsamples_corrected['correction status (reg number)'])\n",
    "BDsamples_corrected.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-13T12:40:47.339665Z",
     "start_time": "2018-05-13T12:40:47.331399Z"
    }
   },
   "outputs": [],
   "source": [
    "BDsamples_corrected['correction status (reg number)'].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-13T12:46:47.660389Z",
     "start_time": "2018-05-13T12:46:47.655875Z"
    }
   },
   "outputs": [],
   "source": [
    "BDsamples_corrected['corrected_regCode']=np.where(BDsamples_corrected['corrected_regCode'].isnull(),BDsamples_corrected['old_RegistrationCode'],BDsamples_corrected['corrected_regCode'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-13T12:46:52.689322Z",
     "start_time": "2018-05-13T12:46:52.662492Z"
    }
   },
   "outputs": [],
   "source": [
    "print BDsamples_corrected[BDsamples_corrected['DnaID']=='BD937']\n",
    "print BDsamples_corrected[BDsamples_corrected['DnaID']=='BD938']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-13T12:47:00.658034Z",
     "start_time": "2018-05-13T12:47:00.609873Z"
    }
   },
   "outputs": [],
   "source": [
    "# add correct UserID:\n",
    "BDsamples_corrected_withUserID=pd.merge(BDsamples_corrected,UserIDs_RegNum,how='left',left_on='corrected_regCode',right_on='RegistrationCode')\n",
    "print len(BDsamples_corrected_withUserID)\n",
    "BDsamples_corrected_withUserID.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-13T12:47:29.454379Z",
     "start_time": "2018-05-13T12:47:29.416977Z"
    }
   },
   "outputs": [],
   "source": [
    "BDsamples_corrected_withUserID[BDsamples_corrected_withUserID['correction status (reg number)']!='same'].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-13T12:48:47.792710Z",
     "start_time": "2018-05-13T12:48:47.786835Z"
    }
   },
   "outputs": [],
   "source": [
    "print len(BDsamples_corrected_withUserID[BDsamples_corrected_withUserID['UserID'].isnull()])\n",
    "print len(BDsamples_corrected_withUserID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-13T12:53:23.429317Z",
     "start_time": "2018-05-13T12:53:23.419429Z"
    }
   },
   "outputs": [],
   "source": [
    "BDsamples_corrected_withUserID=BDsamples_corrected_withUserID[~((BDsamples_corrected_withUserID['UserID']==11343)&(BDsamples_corrected_withUserID['corrected_regCode']==541391))]\n",
    "print 'sample number is still %s' % len(BDsamples_corrected_withUserID)\n",
    "\n",
    "BDsamples_corrected_withUserID = BDsamples_corrected_withUserID[['DnaID', 'storageDT', 'UserID', 'corrected_regCode', 'correction status (reg number)']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "# extract UserID-regNum from the database\n",
    "UserIDs_RegNum = read_sql('select UserID,RegistrationCode from pnp.users', getengine())\n",
    "\n",
    "# merge new UserID to BD sample according to the corrected regNum\n",
    "BDwithCorrectUserID = pd.merge(BDwithCorrectReg, UserIDs_RegNum, how='left',\n",
    "                             left_on='correct registration code',\n",
    "                            right_on='RegistrationCode')\n",
    "# give old UserIDs and regNum to samples that don't have correct regNum:\n",
    "noCorrectReg = len(BDwithCorrectUserID[BDwithCorrectUserID['correct registration code'] == 'unknown'])\n",
    "print '%s samples dont have correct registration code and are given old UserID' % noCorrectReg\n",
    "BDwithCorrectUserID['UserID'] = np.where(BDwithCorrectUserID['correct registration code'] == 'unknown',\n",
    "                                  BDwithCorrectUserID['oldUserID'], BDwithCorrectUserID['UserID'])\n",
    "\n",
    "BDwithCorrectUserID=BDwithCorrectUserID[~((BDwithCorrectUserID['UserID']==11343)&(BDwithCorrectUserID['correct registration code']==541391))]\n",
    "print 'sample number is still %s' % len(BDwithCorrectUserID)\n",
    "\n",
    "BDwithCorrectUserID = BDwithCorrectUserID[['DnaID', 'storageDT', 'UserID', 'correct registration code', 'correction status (reg number)']]\n",
    "\n",
    "# add regNum according to userID and use it only for samples with unknown regNum:\n",
    "BDwithCorrectUserIDandRegs = pd.merge(BDwithCorrectUserID, UserIDs_RegNum, how='left',\n",
    "                                   left_on='UserID', right_on='UserID')\n",
    "BDwithCorrectUserIDandRegs['correct registration code'] = np.where(BDwithCorrectUserIDandRegs['correct registration code'] == 'unknown',\n",
    "                                  BDwithCorrectUserIDandRegs['RegistrationCode'], BDwithCorrectUserIDandRegs['correct registration code'])\n",
    "\n",
    "BDwithCorrectUserIDandRegs = BDwithCorrectUserIDandRegs.drop('RegistrationCode', axis=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.8"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {
    "height": "573px",
    "left": "0px",
    "right": "1533.47px",
    "top": "134px",
    "width": "341px"
   },
   "toc_section_display": "block",
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
