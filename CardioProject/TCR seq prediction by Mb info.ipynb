{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-12T20:24:05.998089Z",
     "start_time": "2018-06-12T20:23:49.866567Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done1\n",
      "stop\n",
      "stop\n",
      "done1\n",
      "stop\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/wisdom/python/lib/python2.7/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from os import listdir,mkdir,makedirs\n",
    "from os.path import isfile, join, isdir,exists\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "from myplots import roundup, rounddown, find_decimal_fold, percentile_cut_off, rarefaction_calc, rarefaction_plot,draw_correlation_scatter\n",
    "from matplotlib.ticker import FormatStrFormatter\n",
    "import cPickle as pickle\n",
    "from Bio.SeqUtils import GC\n",
    "import seaborn as sns\n",
    "import random\n",
    "from scipy.stats import pearsonr,fisher_exact\n",
    "from skbio.diversity.alpha import shannon, simpson, berger_parker_d\n",
    "\n",
    "from pop_organize import get_sample_data, get_sample_with_dfs\n",
    "from SufficientStatistics import *\n",
    "from MyFunctionsShani import *\n",
    "import math\n",
    "from myplots import roundup, rounddown, find_decimal_fold\n",
    "from skbio.stats.distance import mantel\n",
    "from scipy.spatial.distance import braycurtis, pdist\n",
    "from GeneralFeaturePhenotypeInteractions.Feature_phenotype_functions import * \n",
    "from TCR_microbiome_interactions.TCR_microbiome_interactions_functions import *\n",
    "from TCR_microbiome_interactions.TCR_microbiome_interactions_functions2 import *\n",
    "from SampleLists.SampleFileFunctions import *\n",
    "\n",
    "#ML imports:\n",
    "from xgboost import XGBClassifier\n",
    "import lightgbm as lgb\n",
    "from collections import OrderedDict\n",
    "from sklearn.model_selection import GroupKFold,StratifiedKFold, KFold\n",
    "\n",
    "import os\n",
    "from Utils import cacheOnDisk\n",
    "from queue.qp import qp,fakeqp\n",
    "from addloglevels import sethandlers\n",
    "\n",
    "MyPath='/net/mraid08/export/genie/Lab/Personal/ShaniBAF'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-12T20:24:08.010063Z",
     "start_time": "2018-06-12T20:24:07.997380Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'12062018'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time\n",
    "cdate=str(time.strftime(\"%d%m%Y\"))\n",
    "cdate\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare X AND Y TABLES:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-10T06:41:46.260799Z",
     "start_time": "2018-06-10T06:41:46.257389Z"
    }
   },
   "outputs": [],
   "source": [
    "datasetFolder='%s/TCR_real_data/SubSampled15000data_rep2' %MyPath\n",
    "datasetName='PNP515_ss15000_rep2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-10T06:42:33.587618Z",
     "start_time": "2018-06-10T06:42:33.523967Z"
    }
   },
   "outputs": [],
   "source": [
    "# x is the MPA table, in this case, the species, capped to 0.0001 and binary table:\n",
    "MBdf='MPA_s_standardParams_capped0_0001_percShared5_OLtrimmed_binary'\n",
    "MBdfFolder='%s/MicrobiomeDataTables/FilteredAndMergedOnBD' %MyPath\n",
    "file1='%s/%s' %(MBdfFolder,MBdf)\n",
    "MB=pd.read_pickle(file1)\n",
    "MB.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-10T06:42:34.559912Z",
     "start_time": "2018-06-10T06:42:34.492592Z"
    }
   },
   "outputs": [],
   "source": [
    "# Yorig is one column of the TCR, not capped, trimmed, binary, shared by more than 20%:\n",
    "\n",
    "#extract all relevant TCR seqs:\n",
    "TCRdfName='sharingMatrix_PNP515_ss15000_rep2_minNshared2_RA_onlyProductiveTrue__percShared20_OLtrimmed_binary'\n",
    "file2='%s/sharingAnalysis/%s' %(datasetFolder,TCRdfName)\n",
    "TCRdf=pd.read_pickle(file2)\n",
    "TCRdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-10T06:42:35.334273Z",
     "start_time": "2018-06-10T06:42:35.275696Z"
    }
   },
   "outputs": [],
   "source": [
    "# find the most common sequence and take it to be Y\n",
    "\n",
    "TCRdfSorted=TCRdf.copy()\n",
    "TCRdfSorted.loc['sum',:]=TCRdfSorted.sum()\n",
    "TCRdfSorted=TCRdfSorted.sort_values(by='sum',axis=1,ascending=False)\n",
    "TCRdfSorted.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-10T06:42:36.239203Z",
     "start_time": "2018-06-10T06:42:36.225064Z"
    }
   },
   "outputs": [],
   "source": [
    "seq1data=pd.DataFrame(TCRdfSorted.iloc[:-1,0])\n",
    "print len(seq1data)\n",
    "seq1data[-5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-10T06:42:37.119579Z",
     "start_time": "2018-06-10T06:42:37.106665Z"
    }
   },
   "outputs": [],
   "source": [
    "seq2data=pd.DataFrame(TCRdfSorted.iloc[:-1,1])\n",
    "print len(seq2data)\n",
    "seq2data[-5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-10T06:42:37.998646Z",
     "start_time": "2018-06-10T06:42:37.981057Z"
    }
   },
   "outputs": [],
   "source": [
    "seq='CASSLSGSSYNEQFF'\n",
    "seqData=pd.DataFrame(TCRdfSorted.loc[:,seq])\n",
    "print seqData[-5:]\n",
    "seqData=seqData.drop('sum',axis=0)\n",
    "print seqData.shape\n",
    "print seqData[-5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-10T06:42:38.953795Z",
     "start_time": "2018-06-10T06:42:38.948603Z"
    }
   },
   "outputs": [],
   "source": [
    "print len(MB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-10T06:42:39.838392Z",
     "start_time": "2018-06-10T06:42:39.828759Z"
    }
   },
   "outputs": [],
   "source": [
    "# leave only common samples in each df (X and seq1data)\n",
    "MB=MB.loc[[str(x) for x in MB.index.tolist() if x in seqData.index],:]\n",
    "MB=MB.sort_index()\n",
    "X=MB.copy()\n",
    "print len(X)\n",
    "print X.index[100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-10T06:42:40.910819Z",
     "start_time": "2018-06-10T06:42:40.900337Z"
    }
   },
   "outputs": [],
   "source": [
    "seqDatashort=seqData.loc[[str(x) for x in seqData.index.tolist() if x in X.index],:]\n",
    "seqDatashort=seqDatashort.sort_index()\n",
    "print len(seqDatashort)\n",
    "print seqDatashort.index[100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-10T06:42:41.997219Z",
     "start_time": "2018-06-10T06:42:41.984679Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "oldColName=str(seqDatashort.columns.values[0])\n",
    "print oldColName\n",
    "Y=seqDatashort.rename(columns={oldColName:'Class'})\n",
    "Y.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# old stuff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## run predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-10T06:49:18.028988Z",
     "start_time": "2018-06-10T06:47:16.649254Z"
    }
   },
   "outputs": [],
   "source": [
    "seq_MbDF_pairs=[('CASSLSGSSYNEQFF','MPA_s_standardParams_capped0_0001_percShared5_OLtrimmed_binary'),\n",
    "                ('CASSLGQGAYEQYF','MPA_s_standardParams_capped0_0001_percShared5_OLtrimmed_binary'),\n",
    "                ('CASSLAGSYEQYF','MPA_s_standardParams_capped0_0001_percShared5_OLtrimmed_binary')]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-10T06:27:20.548233Z",
     "start_time": "2018-06-10T06:27:20.490977Z"
    }
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.expand_frame_repr', False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# generate X and Y and process them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-12T20:24:09.937922Z",
     "start_time": "2018-06-12T20:24:09.853215Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        CASSLAGSYEQYF\n",
      "Sample               \n",
      "BD944             0.0\n",
      "BD945             1.0\n",
      "BD948             0.0\n",
      "BD949             0.0\n",
      "sum              98.0\n",
      "seqData shape is 392_1\n",
      "        CASSLAGSYEQYF\n",
      "Sample               \n",
      "BD943             0.0\n",
      "BD944             0.0\n",
      "BD945             1.0\n",
      "BD948             0.0\n",
      "BD949             0.0\n",
      "CASSLAGSYEQYF    98.0\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "### prepare Y\n",
    "\n",
    "datasetFolder='%s/TCR_real_data/SubSampled15000data_rep2' %MyPath\n",
    "datasetName='PNP515_ss15000_rep2'\n",
    "TCRdfName='sharingMatrix_PNP515_ss15000_rep2_minNshared2_RA_onlyProductiveTrue__percShared20_OLtrimmed_binary'\n",
    "seq='CASSLAGSYEQYF'\n",
    "YName=TCRdfName.replace('sharingMatrix_PNP515_ss15000_rep2_minNshared2_RA_onlyProductiveTrue__','TCR')\n",
    "#extract TCR:\n",
    "file2='%s/sharingAnalysis/%s' %(datasetFolder,TCRdfName)\n",
    "TCRdf=pd.read_pickle(file2)\n",
    "#extract seq data to be Y:\n",
    "TCRdfSorted=TCRdf.copy()\n",
    "TCRdfSorted.loc['sum',:]=TCRdfSorted.sum()\n",
    "TCRdfSorted=TCRdfSorted.sort_values(by='sum',axis=1,ascending=False)\n",
    "nSharedPerseq=TCRdfSorted.loc['sum',seq]\n",
    "\n",
    "seqData=pd.DataFrame(TCRdfSorted.loc[:,seq])\n",
    "print seqData.tail()\n",
    "seqData=seqData.drop('sum',axis=0)\n",
    "print 'seqData shape is %s_%s' %(seqData.shape[0],seqData.shape[1])\n",
    "Y=seqData.copy()\n",
    "print Y.tail()\n",
    "print Y.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-12T20:24:26.149565Z",
     "start_time": "2018-06-12T20:24:25.933391Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>nTaxa_MPAgclonDivFeatures</th>\n",
       "      <th>meanRA_MPAgclonDivFeatures</th>\n",
       "      <th>meanRAno0_MPAgclonDivFeatures</th>\n",
       "      <th>meadianRAno0_MPAgclonDivFeatures</th>\n",
       "      <th>stdRA_MPAgclonDivFeatures</th>\n",
       "      <th>stdRAno0_MPAgclonDivFeatures</th>\n",
       "      <th>maxRA_MPAgclonDivFeatures</th>\n",
       "      <th>max/meanRA_MPAgclonDivFeatures</th>\n",
       "      <th>max/meanRAno0_MPAgclonDivFeatures</th>\n",
       "      <th>sumTop10_MPAgclonDivFeatures</th>\n",
       "      <th>...</th>\n",
       "      <th>meadianRAno0_MPAsclonDivFeatures</th>\n",
       "      <th>stdRA_MPAsclonDivFeatures</th>\n",
       "      <th>stdRAno0_MPAsclonDivFeatures</th>\n",
       "      <th>maxRA_MPAsclonDivFeatures</th>\n",
       "      <th>max/meanRA_MPAsclonDivFeatures</th>\n",
       "      <th>max/meanRAno0_MPAsclonDivFeatures</th>\n",
       "      <th>sumTop10_MPAsclonDivFeatures</th>\n",
       "      <th>shannon_MPAsclonDivFeatures</th>\n",
       "      <th>simpson_MPAsclonDivFeatures</th>\n",
       "      <th>berger_parker_d_MPAsclonDivFeatures</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BD</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>BD1</th>\n",
       "      <td>38.0</td>\n",
       "      <td>0.003683</td>\n",
       "      <td>0.026312</td>\n",
       "      <td>0.007379</td>\n",
       "      <td>0.022636</td>\n",
       "      <td>0.056806</td>\n",
       "      <td>0.327642</td>\n",
       "      <td>88.962774</td>\n",
       "      <td>12.452272</td>\n",
       "      <td>0.831991</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003768</td>\n",
       "      <td>0.007606</td>\n",
       "      <td>0.022070</td>\n",
       "      <td>0.130465</td>\n",
       "      <td>105.880228</td>\n",
       "      <td>10.808238</td>\n",
       "      <td>0.590882</td>\n",
       "      <td>4.921961</td>\n",
       "      <td>0.947537</td>\n",
       "      <td>0.131446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BD10</th>\n",
       "      <td>37.0</td>\n",
       "      <td>0.003408</td>\n",
       "      <td>0.024952</td>\n",
       "      <td>0.007728</td>\n",
       "      <td>0.017608</td>\n",
       "      <td>0.042841</td>\n",
       "      <td>0.186990</td>\n",
       "      <td>54.872922</td>\n",
       "      <td>7.493872</td>\n",
       "      <td>0.795301</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003070</td>\n",
       "      <td>0.008200</td>\n",
       "      <td>0.026818</td>\n",
       "      <td>0.186990</td>\n",
       "      <td>167.027007</td>\n",
       "      <td>14.066920</td>\n",
       "      <td>0.595756</td>\n",
       "      <td>4.488300</td>\n",
       "      <td>0.924755</td>\n",
       "      <td>0.210586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BD101</th>\n",
       "      <td>39.0</td>\n",
       "      <td>0.003682</td>\n",
       "      <td>0.025636</td>\n",
       "      <td>0.004835</td>\n",
       "      <td>0.017157</td>\n",
       "      <td>0.039628</td>\n",
       "      <td>0.133763</td>\n",
       "      <td>36.325610</td>\n",
       "      <td>5.217860</td>\n",
       "      <td>0.803310</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004448</td>\n",
       "      <td>0.007854</td>\n",
       "      <td>0.023733</td>\n",
       "      <td>0.128536</td>\n",
       "      <td>105.977308</td>\n",
       "      <td>10.045463</td>\n",
       "      <td>0.633085</td>\n",
       "      <td>4.815540</td>\n",
       "      <td>0.941811</td>\n",
       "      <td>0.132990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BD105</th>\n",
       "      <td>37.0</td>\n",
       "      <td>0.003683</td>\n",
       "      <td>0.027019</td>\n",
       "      <td>0.010289</td>\n",
       "      <td>0.018647</td>\n",
       "      <td>0.045053</td>\n",
       "      <td>0.189747</td>\n",
       "      <td>51.523432</td>\n",
       "      <td>7.022749</td>\n",
       "      <td>0.826908</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004267</td>\n",
       "      <td>0.007171</td>\n",
       "      <td>0.020860</td>\n",
       "      <td>0.104872</td>\n",
       "      <td>84.925090</td>\n",
       "      <td>8.143240</td>\n",
       "      <td>0.571824</td>\n",
       "      <td>4.936572</td>\n",
       "      <td>0.952547</td>\n",
       "      <td>0.106816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BD106</th>\n",
       "      <td>28.0</td>\n",
       "      <td>0.003686</td>\n",
       "      <td>0.035709</td>\n",
       "      <td>0.004640</td>\n",
       "      <td>0.035875</td>\n",
       "      <td>0.109642</td>\n",
       "      <td>0.585405</td>\n",
       "      <td>158.797841</td>\n",
       "      <td>16.393941</td>\n",
       "      <td>0.937396</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002706</td>\n",
       "      <td>0.020073</td>\n",
       "      <td>0.075197</td>\n",
       "      <td>0.583773</td>\n",
       "      <td>468.515676</td>\n",
       "      <td>35.040865</td>\n",
       "      <td>0.828480</td>\n",
       "      <td>2.950158</td>\n",
       "      <td>0.648735</td>\n",
       "      <td>0.584585</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 2316 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       nTaxa_MPAgclonDivFeatures  meanRA_MPAgclonDivFeatures  \\\n",
       "BD                                                             \n",
       "BD1                         38.0                    0.003683   \n",
       "BD10                        37.0                    0.003408   \n",
       "BD101                       39.0                    0.003682   \n",
       "BD105                       37.0                    0.003683   \n",
       "BD106                       28.0                    0.003686   \n",
       "\n",
       "       meanRAno0_MPAgclonDivFeatures  meadianRAno0_MPAgclonDivFeatures  \\\n",
       "BD                                                                       \n",
       "BD1                         0.026312                          0.007379   \n",
       "BD10                        0.024952                          0.007728   \n",
       "BD101                       0.025636                          0.004835   \n",
       "BD105                       0.027019                          0.010289   \n",
       "BD106                       0.035709                          0.004640   \n",
       "\n",
       "       stdRA_MPAgclonDivFeatures  stdRAno0_MPAgclonDivFeatures  \\\n",
       "BD                                                               \n",
       "BD1                     0.022636                      0.056806   \n",
       "BD10                    0.017608                      0.042841   \n",
       "BD101                   0.017157                      0.039628   \n",
       "BD105                   0.018647                      0.045053   \n",
       "BD106                   0.035875                      0.109642   \n",
       "\n",
       "       maxRA_MPAgclonDivFeatures  max/meanRA_MPAgclonDivFeatures  \\\n",
       "BD                                                                 \n",
       "BD1                     0.327642                       88.962774   \n",
       "BD10                    0.186990                       54.872922   \n",
       "BD101                   0.133763                       36.325610   \n",
       "BD105                   0.189747                       51.523432   \n",
       "BD106                   0.585405                      158.797841   \n",
       "\n",
       "       max/meanRAno0_MPAgclonDivFeatures  sumTop10_MPAgclonDivFeatures  \\\n",
       "BD                                                                       \n",
       "BD1                            12.452272                      0.831991   \n",
       "BD10                            7.493872                      0.795301   \n",
       "BD101                           5.217860                      0.803310   \n",
       "BD105                           7.022749                      0.826908   \n",
       "BD106                          16.393941                      0.937396   \n",
       "\n",
       "                      ...                   meadianRAno0_MPAsclonDivFeatures  \\\n",
       "BD                    ...                                                      \n",
       "BD1                   ...                                           0.003768   \n",
       "BD10                  ...                                           0.003070   \n",
       "BD101                 ...                                           0.004448   \n",
       "BD105                 ...                                           0.004267   \n",
       "BD106                 ...                                           0.002706   \n",
       "\n",
       "       stdRA_MPAsclonDivFeatures  stdRAno0_MPAsclonDivFeatures  \\\n",
       "BD                                                               \n",
       "BD1                     0.007606                      0.022070   \n",
       "BD10                    0.008200                      0.026818   \n",
       "BD101                   0.007854                      0.023733   \n",
       "BD105                   0.007171                      0.020860   \n",
       "BD106                   0.020073                      0.075197   \n",
       "\n",
       "       maxRA_MPAsclonDivFeatures  max/meanRA_MPAsclonDivFeatures  \\\n",
       "BD                                                                 \n",
       "BD1                     0.130465                      105.880228   \n",
       "BD10                    0.186990                      167.027007   \n",
       "BD101                   0.128536                      105.977308   \n",
       "BD105                   0.104872                       84.925090   \n",
       "BD106                   0.583773                      468.515676   \n",
       "\n",
       "       max/meanRAno0_MPAsclonDivFeatures  sumTop10_MPAsclonDivFeatures  \\\n",
       "BD                                                                       \n",
       "BD1                            10.808238                      0.590882   \n",
       "BD10                           14.066920                      0.595756   \n",
       "BD101                          10.045463                      0.633085   \n",
       "BD105                           8.143240                      0.571824   \n",
       "BD106                          35.040865                      0.828480   \n",
       "\n",
       "       shannon_MPAsclonDivFeatures  simpson_MPAsclonDivFeatures  \\\n",
       "BD                                                                \n",
       "BD1                       4.921961                     0.947537   \n",
       "BD10                      4.488300                     0.924755   \n",
       "BD101                     4.815540                     0.941811   \n",
       "BD105                     4.936572                     0.952547   \n",
       "BD106                     2.950158                     0.648735   \n",
       "\n",
       "       berger_parker_d_MPAsclonDivFeatures  \n",
       "BD                                          \n",
       "BD1                               0.131446  \n",
       "BD10                              0.210586  \n",
       "BD101                             0.132990  \n",
       "BD105                             0.106816  \n",
       "BD106                             0.584585  \n",
       "\n",
       "[5 rows x 2316 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# PREPARE x\n",
    "folder='%s/MicrobiomeDataTables/MbFeatureTables' %MyPath\n",
    "tabType='notFiltered'\n",
    "\n",
    "file2='%s/%s_mergedFeatureTable' %(folder,tabType)\n",
    "mergedFeatureTable=pd.read_pickle(file2)\n",
    "\n",
    "X=mergedFeatureTable.copy()\n",
    "XName='mergedFeatureTable_%s' %tabType\n",
    "\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-12T20:24:36.724884Z",
     "start_time": "2018-06-12T20:24:36.677114Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape is 371_2316\n",
      "the 100th sample in X is BD375\n",
      "       nTaxa_MPAgclonDivFeatures  meanRA_MPAgclonDivFeatures  \\\n",
      "BD                                                             \n",
      "BD1                         38.0                    0.003683   \n",
      "BD10                        37.0                    0.003408   \n",
      "BD101                       39.0                    0.003682   \n",
      "\n",
      "       meanRAno0_MPAgclonDivFeatures  \n",
      "BD                                    \n",
      "BD1                         0.026312  \n",
      "BD10                        0.024952  \n",
      "BD101                       0.025636  \n",
      "Y shape is 371_1\n",
      "the 100th sample in Y is BD375\n",
      "        Class\n",
      "Sample       \n",
      "BD1       1.0\n",
      "BD10      0.0\n",
      "BD101     0.0\n"
     ]
    }
   ],
   "source": [
    "#co-processing of X and Y\n",
    "\n",
    "X=X.loc[[str(x) for x in X.index.tolist() if x in Y.index],:]\n",
    "X=X.sort_index()\n",
    "print 'X shape is %s_%s' %(X.shape[0], X.shape[1])\n",
    "print 'the 100th sample in X is %s' %X.index[100]\n",
    "print X.iloc[:3,:3]\n",
    "Y=Y.loc[[str(x) for x in Y.index.tolist() if x in X.index],:]\n",
    "Y=Y.sort_index()\n",
    "oldColName=str(Y.columns.values[0])\n",
    "Y=Y.rename(columns={oldColName:'Class'})\n",
    "print 'Y shape is %s_%s' %(Y.shape[0], Y.shape[1])\n",
    "print 'the 100th sample in Y is %s' %Y.index[100]\n",
    "print Y.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# develop train_test_split design:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-12T08:38:49.945224Z",
     "start_time": "2018-06-12T08:38:49.934758Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split  \n",
    "# seed = 7\n",
    "test_size = 0.33\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=test_size, stratify=Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-12T08:39:11.729048Z",
     "start_time": "2018-06-12T08:39:10.884678Z"
    }
   },
   "outputs": [],
   "source": [
    "model = XGBClassifier()\n",
    "model.fit(X_train, y_train)\n",
    "# make predictions for test data\n",
    "y_pred = model.predict(X_test)\n",
    "print y_pred\n",
    "predictions = [round(value) for value in y_pred]\n",
    "print predictions\n",
    "# evaluate predictions\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "accuracy2 = accuracy_score(y_test, predictions)\n",
    "print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))\n",
    "print(\"Accuracy2: %.2f%%\" % (accuracy2 * 100.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# improve CV splitting development:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-06-12T20:06:50.118Z"
    }
   },
   "outputs": [],
   "source": [
    "useCV=True\n",
    "stratifiedCV=False\n",
    "n_splits=3\n",
    "model= lgb.LGBMClassifier\n",
    "model_params={'n_estimators':20, 'learning_rate': 0.1,'max_depth':3,'numthreads':2}\n",
    "\n",
    "##### copy from here to the function####\n",
    "#convert X and Y to arrays so they will work with the StratifiedKFold method\n",
    "X1=np.array(X.values)\n",
    "Y1=np.array(Y.values)\n",
    "Y1=np.reshape(Y1,[Y1.shape[0],])\n",
    "print X1.shape\n",
    "print Y1.shape\n",
    "\n",
    "if useCV:\n",
    "    print 'splitting train_test using cross validation with %s splits...' %n_splits\n",
    "#     groups = np.array(range(X1.shape[0]))\n",
    "    if stratifiedCV:\n",
    "        kf = StratifiedKFold(n_splits=n_splits)\n",
    "    else:\n",
    "        kf = KFold(n_splits=n_splits)\n",
    "\n",
    "    y_pred_df = pd.DataFrame(index=Y.index, columns=['pred_proba','y_pred'])\n",
    "    i=0\n",
    "    for train_index,val_index in kf.split(X1, Y1):\n",
    "        print i\n",
    "        i+=1\n",
    "        X_train, X_val = X1[train_index], X1[val_index]\n",
    "        y_train, y_val = Y1[train_index], Y1[val_index]\n",
    "        \n",
    "        print 'y_train mean=%s' %y_train.mean()\n",
    "        print 'y_val mean=%s' %y_val.mean()\n",
    "\n",
    "        # creating the model object\n",
    "        m = model(**model_params)\n",
    "\n",
    "        # fitting the training\n",
    "        print 'fitting the model'\n",
    "        m.fit(X_train, y_train,\n",
    "                eval_set=[(X_val, y_val)],\n",
    "                early_stopping_rounds=None, verbose=-1)\n",
    "        # getting the predictions for the test\n",
    "        print 'getting predictions....'\n",
    "        y_pred_proba = m.predict_proba(X_val)\n",
    "        predictions = m.predict(X_val)\n",
    "        inds=[x for x in range(len(Y.index)) if x in val_index]\n",
    "\n",
    "        y_pred_df.iloc[inds, :] = np.expand_dims(y_pred_proba[:,1], 1)\n",
    "        y_pred_df.iloc[inds, 1] = predictions\n",
    "        \n",
    "        print 'y_pred_df_head:'\n",
    "        print y_pred_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# develop: gridSearchCV:\n",
    "THIS PART SHOULD BE A SEPERATE FUNCTION:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-06-12T21:20:40.267Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(371, 2316)\n",
      "(371,)\n",
      "[[2.90000000e+01 3.68592950e-03 3.44754621e-02 ... 4.65342738e+00\n",
      "  9.37595017e-01 1.45896657e-01]\n",
      " [3.70000000e+01 3.67845980e-03 2.69868061e-02 ... 4.90275927e+00\n",
      "  9.51480783e-01 1.03377687e-01]\n",
      " [3.60000000e+01 3.24589436e-03 2.43932953e-02 ... 4.96175612e+00\n",
      "  9.53365480e-01 1.11111111e-01]\n",
      " ...\n",
      " [3.80000000e+01 3.63969736e-03 2.59956807e-02 ... 5.20228919e+00\n",
      "  9.61758432e-01 8.91393443e-02]\n",
      " [3.20000000e+01 3.68424654e-03 3.12381418e-02 ... 2.54579389e+00\n",
      "  5.82526273e-01 6.39393939e-01]\n",
      " [4.00000000e+01 3.62233774e-03 2.45802473e-02 ... 4.82329050e+00\n",
      "  9.50170762e-01 1.06122449e-01]]\n",
      "[0. 0. 1. 1. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 1.\n",
      " 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.\n",
      " 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0.\n",
      " 1. 0. 0. 1. 1. 1. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 1. 0.\n",
      " 0. 0. 0. 0. 1. 0. 0. 1. 1. 0. 1. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0.\n",
      " 1. 0. 1. 1. 1. 1. 0. 0. 0. 1. 0. 0. 0. 1. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 1. 1. 1. 0. 0. 0. 0.]\n",
      "# Tuning hyper-parameters for precision\n",
      "()\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "model= lgb.LGBMClassifier\n",
    "#convert X and Y to arrays so they will work with the StratifiedKFold method\n",
    "X1=np.array(X.values)\n",
    "Y1=np.array(Y.values)\n",
    "Y1=np.reshape(Y1,[Y1.shape[0],])\n",
    "print X1.shape\n",
    "print Y1.shape\n",
    "\n",
    "# Split the dataset in two equal parts\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X1, Y1, test_size=0.5, random_state=0)\n",
    "print X_train\n",
    "print y_train\n",
    "\n",
    "# Set the parameters by cross-validation\n",
    "tuned_parameters =[{'n_estimators':[20,100,1000], 'learning_rate': [0.01,0.05,0.1,1],'max_depth':[-1,3,5]}]\n",
    "\n",
    "scores = ['precision', 'recall']\n",
    "\n",
    "for score in scores:\n",
    "    print(\"# Tuning hyper-parameters for %s\" % score)\n",
    "    print()\n",
    "\n",
    "    clf = GridSearchCV(model(), tuned_parameters, cv=5,\n",
    "                       scoring='%s_macro' % score,n_jobs=32)\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    print(\"Best parameters set found on development set:\")\n",
    "    print()\n",
    "    print(clf.best_params_)\n",
    "    print()\n",
    "    print(\"Grid scores on development set:\")\n",
    "    print()\n",
    "    means = clf.cv_results_['mean_test_score']\n",
    "    stds = clf.cv_results_['std_test_score']\n",
    "    for mean, std, params in zip(means, stds, clf.cv_results_['params']):\n",
    "        print(\"%0.3f (+/-%0.03f) for %r\"\n",
    "              % (mean, std * 2, params))\n",
    "    print()\n",
    "\n",
    "    print(\"Detailed classification report:\")\n",
    "    print()\n",
    "    print(\"The model is trained on the full development set.\")\n",
    "    print(\"The scores are computed on the full evaluation set.\")\n",
    "    print()\n",
    "    y_true, y_pred = y_test, clf.predict(X_test)\n",
    "    print(classification_report(y_true, y_pred))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# develop - "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model= lgb.LGBMClassifier\n",
    "#convert X and Y to arrays so they will work with the StratifiedKFold method\n",
    "X1=np.array(X.values)\n",
    "Y1=np.array(Y.values)\n",
    "Y1=np.reshape(Y1,[Y1.shape[0],])\n",
    "print X1.shape\n",
    "print Y1.shape\n",
    "\n",
    "\n",
    "original_params = {'n_estimators': 1000, 'max_leaf_nodes': 4, 'max_depth': None, 'random_state': 2,\n",
    "                   'min_samples_split': 5}\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X1, Y1, test_size=0.33, random_state=0)\n",
    "plt.figure()\n",
    "\n",
    "for label, color, setting in [('No shrinkage', 'orange',\n",
    "                               {'learning_rate': 1.0, 'subsample': 1.0}),\n",
    "                              ('learning_rate=0.1', 'turquoise',\n",
    "                               {'learning_rate': 0.1, 'subsample': 1.0}),\n",
    "                              ('subsample=0.5', 'blue',\n",
    "                               {'learning_rate': 1.0, 'subsample': 0.5}),\n",
    "                              ('learning_rate=0.1, subsample=0.5', 'gray',\n",
    "                               {'learning_rate': 0.1, 'subsample': 0.5}),\n",
    "                              ('learning_rate=0.1, max_features=2', 'magenta',\n",
    "                               {'learning_rate': 0.1, 'max_features': 2})]:\n",
    "    \n",
    "    params = dict(original_params)\n",
    "    params.update(setting)\n",
    "\n",
    "    clf = ensemble.GradientBoostingClassifier(**params)\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    # compute test set deviance\n",
    "    test_deviance = np.zeros((params['n_estimators'],), dtype=np.float64)\n",
    "\n",
    "    for i, y_pred in enumerate(clf.staged_decision_function(X_test)):\n",
    "        # clf.loss_ assumes that y_test[i] in {0, 1}\n",
    "        test_deviance[i] = clf.loss_(y_test, y_pred)\n",
    "\n",
    "    plt.plot((np.arange(test_deviance.shape[0]) + 1)[::5], test_deviance[::5],\n",
    "            '-', color=color, label=label)\n",
    "\n",
    "plt.legend(loc='upper left')\n",
    "plt.xlabel('Boosting Iterations')\n",
    "plt.ylabel('Test Set Deviance')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# define function to use any model and parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-12T10:55:04.049494Z",
     "start_time": "2018-06-12T10:55:03.530912Z"
    }
   },
   "outputs": [],
   "source": [
    "def predictBinaryByDistMat(Y,YName,X,XName,ResultFolder,model,model_name,model_params,n_splits,useCV=True,stratifiedCV=True):\n",
    "    \n",
    "    #(1) arrange result folders:\n",
    "    print 'arranging  result folders:'\n",
    "    predResultsDF=pd.DataFrame()\n",
    "    \n",
    "    predResultsDFFolder='%s/predictionDFs' %ResultFolder #define folder for all result dfs in this model\n",
    "    if not isdir(predResultsDFFolder):\n",
    "        makedirs(predResultsDFFolder)\n",
    "        print 'generating predResultsDFFolder %s' %predResultsDFFolder\n",
    "        \n",
    "    predResultsfigFolder='%s/figs' %ResultFolder #define folder for figs in this model\n",
    "    if not isdir(predResultsfigFolder):\n",
    "        makedirs(predResultsfigFolder)\n",
    "        print 'predResultsfigFolder %s' %predResultsfigFolder\n",
    "\n",
    "    #generate result DF name based on model_params:\n",
    "    d=OrderedDict(sorted(model_params.items(), key=lambda t: t[0]))\n",
    "    try:\n",
    "        d2=d.copy()\n",
    "        del d2['numthreads']\n",
    "    except:\n",
    "        d2=d.copy()\n",
    "    predResultsDFName='_'.join(['%s%s' %(key.replace('_',''), value) for (key, value) in d2.items()]) #generate a file name based on params\n",
    "    predResultsDFName=predResultsDFName.replace('.','-')\n",
    "    if useCV:\n",
    "        predResultsDFName='%s_CV%s' %(predResultsDFName,n_splits)\n",
    "    \n",
    "    predResultsDFfile='%s/%s' %(predResultsDFFolder,predResultsDFName)\n",
    "    existingDFs=[f for f in listdir( predResultsDFFolder) if isfile(join(predResultsDFFolder, f))]\n",
    "    \n",
    "\n",
    "    if predResultsDFName not in existingDFs:  \n",
    "        \n",
    "\n",
    "        # common processing of X and y:\n",
    "        # leave only common samples in each df (X and seq1data)\n",
    "        X=X.loc[[str(x) for x in X.index.tolist() if x in Y.index],:]\n",
    "        X=X.sort_index()\n",
    "        print 'X shape is %s_%s' %(X.shape[0], X.shape[1])\n",
    "        print 'the 100th sample in X is %s' %X.index[100]\n",
    "        print X.iloc[:3,:3]\n",
    "        Y=Y.loc[[str(x) for x in Y.index.tolist() if x in X.index],:]\n",
    "        Y=Y.sort_index()\n",
    "        oldColName=str(Y.columns.values[0])\n",
    "        Y=Y.rename(columns={oldColName:'Class'})\n",
    "        print 'Y shape is %s_%s' %(Y.shape[0], Y.shape[1])\n",
    "        print 'the 100th sample in Y is %s' %Y.index[100]\n",
    "        print Y.head(3)\n",
    "\n",
    "        #(2) model fitting and predictions:\n",
    "\n",
    "        \n",
    "        if useCV:\n",
    "            print 'splitting train_test using cross validation with %s splits...' %n_splits\n",
    "            groups = np.array(range(X.shape[0]))\n",
    "            if stratifiedCV:\n",
    "                group_kfold = StratifiedKFold(n_splits=n_splits)\n",
    "            else:\n",
    "                group_kfold = GroupKFold(n_splits=n_splits)\n",
    "        \n",
    "            y_pred_df = pd.DataFrame(index=Y.index, columns=['pred_proba'])\n",
    "            i=0\n",
    "            for train_index, val_index in group_kfold.split(X, Y, groups):\n",
    "                print i\n",
    "                i+=1\n",
    "                X_train, X_val = X.iloc[train_index], X.iloc[val_index]\n",
    "                y_train, y_val = Y.loc[X_train.index].Class, Y.loc[X_val.index].Class\n",
    "\n",
    "                # creating the model object\n",
    "                m = model(**model_params)\n",
    "\n",
    "                # fitting the training\n",
    "                m.fit(X_train, y_train,\n",
    "                        eval_set=[(X_val, y_val)],\n",
    "                        early_stopping_rounds=None, verbose=-1)\n",
    "                # getting the predictions for the test\n",
    "                y_pred_proba = m.predict_proba(X_val)\n",
    "                y_pred_df.loc[y_val.index, :] = np.expand_dims(y_pred_proba[:,1], 1)\n",
    "        else:\n",
    "            test_size = 0.33\n",
    "            print 'using normal train_test split with test_size=%s' %test_size\n",
    "            from sklearn.model_selection import train_test_split  \n",
    "            # seed = 7\n",
    "            \n",
    "            X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=test_size, stratify=Y)\n",
    "            print 'fitting model...'\n",
    "            model = model\n",
    "            model.fit(X_train, y_train)\n",
    "            # make predictions for test data\n",
    "            print 'predicting...'\n",
    "            y_pred = model.predict(X_test)\n",
    "            predictions = [round(value) for value in y_pred]\n",
    "            # evaluate predictions\n",
    "            from sklearn.metrics import accuracy_score\n",
    "            accuracy = accuracy_score(y_test, y_pred)\n",
    "            print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))\n",
    "            y_pred_df = pd.DataFrame(index=Y.index, data={'pred':y_pred})\n",
    "\n",
    "        #(4) plots:\n",
    "        print 'generating plots...'\n",
    "\n",
    "        # this plot shows the probabilities returned by the predictor colored by the class\n",
    "        plt.figure(figsize=(3,2))\n",
    "        plt.scatter(range(y_pred_df.shape[0]), y_pred_df.pred_proba, c=Y.Class)\n",
    "        plt.show()\n",
    "\n",
    "        # plot ROC and PR curves\n",
    "\n",
    "        from sklearn import metrics\n",
    "        fpr, tpr, thresholds = metrics.roc_curve(Y.Class+1, y_pred_df.pred_proba, pos_label=2)\n",
    "        roc_auc = metrics.auc(fpr, tpr)\n",
    "        print roc_auc\n",
    "\n",
    "        plt.figure(figsize=(20,7))\n",
    "        plt.subplot(1,2,1)\n",
    "        lw = 2\n",
    "        plt.plot(fpr, tpr, color='darkorange', lw=lw, label='ROC curve (area = %0.3f)' % roc_auc)\n",
    "        plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "        plt.xlim([0.0, 1.0])\n",
    "        plt.ylim([0.0, 1.05])\n",
    "        plt.xlabel('False Positive Rate', fontsize=15)\n",
    "        plt.ylabel('True Positive Rate', fontsize=15)\n",
    "        plt.title('ROC curve - ' , fontsize=20)\n",
    "        plt.legend(loc=\"lower right\", fontsize=15)\n",
    "\n",
    "        plt.subplot(1,2,2)\n",
    "        precision, recall, _ = metrics.precision_recall_curve(Y.Class, y_pred_df.pred)\n",
    "        pr_auc = metrics.auc(recall, precision)\n",
    "\n",
    "        plt.step(recall, precision, color='b', alpha=0.2, where='post')\n",
    "        plt.fill_between(recall, precision, step='post', alpha=0.5,\n",
    "                         color='darkorange', label='Precision Recall curve - AUC = {0:0.3f}'.format(pr_auc))\n",
    "        plt.plot([0, 1], [Y.sum()/Y.shape[0], Y.sum()/Y.shape[0]], color='navy', lw=lw, linestyle='--')\n",
    "        plt.xlabel('Recall', fontsize=15)\n",
    "        plt.ylabel('Precision', fontsize=15)\n",
    "        plt.ylim([0.0, 1.05])\n",
    "        plt.xlim([0.0, 1.0])\n",
    "        plt.title('Precision Recall curve', fontsize=20)\n",
    "        plt.legend(loc=\"upper right\", fontsize=15)\n",
    "\n",
    "        if roc_auc>0.55:\n",
    "            predResultsFigfile='%s/%s_ROC-PR.png' %(predResultsfigFolder,predResultsDFName)\n",
    "            plt.savefig(predResultsFigfile, bbox_inches='tight')\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "        #(5) generate summarizing df:\n",
    "        print 'generating summarizing df'\n",
    "        predResultsDF.loc[0,'Yname']=YName\n",
    "        predResultsDF.loc[0,'Xname']=XName\n",
    "        preResultsDF.loc[0,'model_name']=model_name\n",
    "        predResultsDF.loc[0,'roc_auc']=round(roc_auc,3)\n",
    "        predResultsDF.loc[0,'pr_auc']=round(pr_auc,3)\n",
    "        predResultsDF.loc[0,'useCV']=useCV\n",
    "        if useCV:\n",
    "            predResultsDF.loc[0,'n_splits']=n_splits\n",
    "        \n",
    "        \n",
    "        for (key, value) in OrderedDict(sorted(model_params.items(), key=lambda t: t[0])).items():\n",
    "            predResultsDF.loc[0,key]=value\n",
    "        \n",
    "        nPos=int(Y.Class.sum())\n",
    "        nNeg=len(Y)-nPos\n",
    "        \n",
    "        print nPos, nNeg\n",
    "\n",
    "        predResultsDF.loc[0,'nPos']=nPos\n",
    "        predResultsDF.loc[0,'nNeg']=nNeg\n",
    "        \n",
    "        \n",
    "        predResultsDF.to_pickle(predResultsDFfile)\n",
    "        print predResultsDFfile\n",
    "        \n",
    "    else:\n",
    "        print 'this prediction already exists'\n",
    "    \n",
    "    return predResultsDF\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## run with default parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-12T10:55:08.500297Z",
     "start_time": "2018-06-12T10:55:08.490139Z"
    }
   },
   "outputs": [],
   "source": [
    "### prepare seqData\n",
    "\n",
    "datasetFolder='%s/TCR_real_data/SubSampled15000data_rep2' %MyPath\n",
    "datasetName='PNP515_ss15000_rep2'\n",
    "TCRdfName='sharingMatrix_PNP515_ss15000_rep2_minNshared2_RA_onlyProductiveTrue__percShared20_OLtrimmed_binary'\n",
    "seq='CASSLAGSYEQYF'\n",
    "YName=TCRdfName.replace('sharingMatrix_PNP515_ss15000_rep2_minNshared2_RA_onlyProductiveTrue__','TCR')\n",
    "#extract TCR:\n",
    "file2='%s/sharingAnalysis/%s' %(datasetFolder,TCRdfName)\n",
    "TCRdf=pd.read_pickle(file2)\n",
    "#extract seq data to be Y:\n",
    "TCRdfSorted=TCRdf.copy()\n",
    "TCRdfSorted.loc['sum',:]=TCRdfSorted.sum()\n",
    "TCRdfSorted=TCRdfSorted.sort_values(by='sum',axis=1,ascending=False)\n",
    "nSharedPerseq=TCRdfSorted.loc['sum',seq]\n",
    "\n",
    "seqData=pd.DataFrame(TCRdfSorted.loc[:,seq])\n",
    "print seqData.tail()\n",
    "seqData=seqData.drop('sum',axis=0)\n",
    "print 'seqData shape is %s_%s' %(seqData.shape[0],seqData.shape[1])\n",
    "Y=seqData.copy()\n",
    "print Y.tail()\n",
    "print Y.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-12T10:55:13.038926Z",
     "start_time": "2018-06-12T10:55:13.006189Z"
    }
   },
   "outputs": [],
   "source": [
    "file1='%s/%s' %(MBdfFolder,MBdfName)\n",
    "MB=pd.read_pickle(file1)\n",
    "X=MB.copy()\n",
    "\n",
    "#extract TCR:\n",
    "file2='%s/sharingAnalysis/%s' %(datasetFolder,TCRdfName)\n",
    "TCRdf=pd.read_pickle(file2)\n",
    "#extract seq data to be Y:\n",
    "TCRdfSorted=TCRdf.copy()\n",
    "TCRdfSorted.loc['sum',:]=TCRdfSorted.sum()\n",
    "TCRdfSorted=TCRdfSorted.sort_values(by='sum',axis=1,ascending=False)\n",
    "nSharedPerseq=TCRdfSorted.loc['sum',seq]\n",
    "\n",
    "seqData=pd.DataFrame(TCRdfSorted.loc[:,seq])\n",
    "print seqData.tail()\n",
    "seqData=seqData.drop('sum',axis=0)\n",
    "print 'seqData shape is %s_%s' %(seqData.shape[0],seqData.shape[1])\n",
    "Y=seqData.copy()\n",
    "print Y.tail()\n",
    "print Y.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-12T10:55:17.605386Z",
     "start_time": "2018-06-12T10:55:17.486447Z"
    }
   },
   "outputs": [],
   "source": [
    "ResultFolder='%s/TCR_mb_results/seqTCRpredictions' %datasetFolder\n",
    "model= lgb.LGBMClassifier\n",
    "model_name='LGBMClassifier'\n",
    "n_splits=5\n",
    "model_params={}\n",
    "predResultsDF=predictBinaryByDistMat(Y,YName,X,XName,ResultFolder,model,model_name,model_params,n_splits=n_splits,useCV=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-10T10:37:18.666622Z",
     "start_time": "2018-06-10T10:35:13.952407Z"
    }
   },
   "outputs": [],
   "source": [
    "ResultFolder='%s/TCR_mb_results/seqTCRpredictions' %datasetFolder\n",
    "model= lgb.LGBMClassifier\n",
    "model_name='LGBMClassifier'\n",
    "learning_rate_list=[0.01,0.05,0.1,0.2]\n",
    "n_estimators_list=[10,20,50,100]\n",
    "max_depth_list=[-1,2,3,4]\n",
    "n_splits_list=[5,10]\n",
    "\n",
    "count=0\n",
    "for learning_rate in learning_rate_list:\n",
    "    for n_estimators in n_estimators_list:\n",
    "        for max_depth in max_depth_list:\n",
    "            for n_splits in n_splits_list:\n",
    "                model_params={'learning_rate': learning_rate, 'n_estimators': n_estimators,'max_depth': max_depth,'num_threads':2}\n",
    "\n",
    "                print count,learning_rate,n_estimators, max_depth, n_splits\n",
    "\n",
    "                predResultsDF=predictBinaryByDistMat(Y,YName,X,XName,ResultFolder,model,model_name,model_params,n_splits=n_splits,useCV=True)\n",
    "                \n",
    "                count=count+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-10T09:19:59.995446Z",
     "start_time": "2018-06-10T09:19:59.970676Z"
    }
   },
   "outputs": [],
   "source": [
    "predResultsDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## get results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-06-10T09:33:41.577Z"
    }
   },
   "outputs": [],
   "source": [
    "modelResultFolder='%s/%s' %(ResultFolder,model_name) #define folder for all model results\n",
    "predResultsDFFolder='%s/predictionDFs' %modelResultFolder #define folder for all result dfs in this model\n",
    "\n",
    "predResults_lgb=concat_summarizing_dfs(predResultsDFFolder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-06-10T09:33:42.018Z"
    }
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.expand_frame_repr', False)\n",
    "predResults_lgb=predResults_lgb.sort_values(by='roc_auc',ascending=False)\n",
    "predResults_lgb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## run with many features:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### prepare x and y:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-11T21:22:13.260489Z",
     "start_time": "2018-06-11T21:22:13.227045Z"
    }
   },
   "outputs": [],
   "source": [
    "### prepare seqData\n",
    "\n",
    "datasetFolder='%s/TCR_real_data/SubSampled15000data_rep2' %MyPath\n",
    "datasetName='PNP515_ss15000_rep2'\n",
    "TCRdfName='sharingMatrix_PNP515_ss15000_rep2_minNshared2_RA_onlyProductiveTrue__percShared20_OLtrimmed_binary'\n",
    "seq='CASSLAGSYEQYF'\n",
    "YName=TCRdfName.replace('sharingMatrix_PNP515_ss15000_rep2_minNshared2_RA_onlyProductiveTrue__','TCR')\n",
    "#extract TCR:\n",
    "file2='%s/sharingAnalysis/%s' %(datasetFolder,TCRdfName)\n",
    "TCRdf=pd.read_pickle(file2)\n",
    "#extract seq data to be Y:\n",
    "TCRdfSorted=TCRdf.copy()\n",
    "TCRdfSorted.loc['sum',:]=TCRdfSorted.sum()\n",
    "TCRdfSorted=TCRdfSorted.sort_values(by='sum',axis=1,ascending=False)\n",
    "nSharedPerseq=TCRdfSorted.loc['sum',seq]\n",
    "\n",
    "seqData=pd.DataFrame(TCRdfSorted.loc[:,seq])\n",
    "print seqData.tail()\n",
    "seqData=seqData.drop('sum',axis=0)\n",
    "print 'seqData shape is %s_%s' %(seqData.shape[0],seqData.shape[1])\n",
    "Y=seqData.copy()\n",
    "print Y.tail()\n",
    "print Y.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-11T21:32:53.088838Z",
     "start_time": "2018-06-11T21:32:52.986303Z"
    }
   },
   "outputs": [],
   "source": [
    "folder='%s/MicrobiomeDataTables/MbFeatureTables' %MyPath\n",
    "tabType='notFiltered'\n",
    "\n",
    "file2='%s/%s_mergedFeatureTable' %(folder,tabType)\n",
    "mergedFeatureTable=pd.read_pickle(file2)\n",
    "\n",
    "X=mergedFeatureTable.copy()\n",
    "XName='mergedFeatureTable_%s' %tabType\n",
    "\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### run model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-11T21:40:40.703619Z",
     "start_time": "2018-06-11T21:40:36.393863Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ResultFolder='%s/TCR_mb_results/seqTCRpredictions/manyFeaturesNotFiltered' %datasetFolder\n",
    "model= lgb.LGBMClassifier\n",
    "model_name='LGBMClassifier'\n",
    "# learning_rate_list=[0.01,0.05,0.1,0.2]\n",
    "# n_estimators_list=[10,20,50,100]\n",
    "# max_depth_list=[-1,2,3,4]\n",
    "# n_splits_list=[5,10]\n",
    "\n",
    "learning_rate=0.1\n",
    "n_estimators=100\n",
    "max_depth=3\n",
    "n_splits=10\n",
    "\n",
    "model_params={'learning_rate': learning_rate, 'n_estimators': n_estimators,'max_depth': max_depth,'num_threads':2}\n",
    "predResultsDF=predictBinaryByDistMat(Y,YName,X,XName,ResultFolder,model,model_name,model_params,n_splits=n_splits,useCV=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.8"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {
    "height": "503px",
    "left": "0px",
    "right": "1877.34px",
    "top": "134px",
    "width": "299px"
   },
   "toc_section_display": "block",
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
